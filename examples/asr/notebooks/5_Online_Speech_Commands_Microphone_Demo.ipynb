{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates speech command recognition from a microphone's stream in NeMo.\n",
    "\n",
    "It is **not a recommended** way to do inference in production workflows. If you are interested in \n",
    "production-level inference using NeMo ASR models, please sign-up to Jarvis early access program: https://developer.nvidia.com/nvidia-jarvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook requires PyAudio library to get a signal from an audio device.\n",
    "For Ubuntu, please run the following commands to install it:\n",
    "```\n",
    "sudo apt-get install -y portaudio19-dev\n",
    "pip install pyaudio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import numpy as np\n",
    "import pyaudio as pa\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture and Weights\n",
    "\n",
    "The model architecture is defined in a YAML file available in the config directory. MatchboxNet 3x1x64 has been trained on the Google Speech Commands dataset (v2) version, and these weights are available on NGC. They will automatically be downloaded if not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the checkpoints are available from NGC: https://ngc.nvidia.com/catalog/models/nvidia:google_speech_commands_v2___matchboxnet_3x1x1\n",
    "MODEL_YAML = '../configs/quartznet_speech_commands_3x1_v2.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-18 03:21:32--  https://api.ngc.nvidia.com/v2/models/nvidia/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperEncoder-STEP-89000.pt\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 54.241.185.37, 54.153.26.224\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|54.241.185.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperEncoder-STEP-89000.pt?response-content-disposition=attachment%3B%20filename%3D%22JasperEncoder-STEP-89000.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJGMEQCIBk0tZOxMgN%2Bc6qMu3rqTITghZaj6pu6efQcbZ5KxeB7AiAPGZKlEHCSp7Wbh7i1W1seyVNNjrjSFvfQyMVX9DocFyq9AwjS%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDc4OTM2MzEzNTAyNyIM5FRiuDuzmFo9QSawKpEDZ6PeqYd0OC5B%2FzzCGgGEHwyuPgg%2B5WifOHqvRFFIb5b8Nl0RiiVWQyktqF6V%2BUx8nG2TIhPqftMMBgrV7kOHwd853SIy21qij4QLqa8m0%2FWHHvjPRd0ZUE0FMcr%2BgkLtmmOC6XDa3hDPWLigccfJYgesuh7OX%2FDnoCq0P7JVw%2BrihE6okulpAzLlFenoDb7GbFztdph%2Buw2amD%2Bt2BZqCC2DqTsASFkd9DVITOITcfsKjDBz%2BVFVvRIQKl7MsVEqSxtXBTatr5ODzjJ1yiNVylUy3tlm%2F1oOKAo4xPJD7lrhAgHknTE%2Bd7xDy%2Bu1Cr54okMrh%2F9tMlPMbrscY4O1uyXQIU651tXaKkNLpYCcji3Ajmp0jFU0Wc74njLhO6570fvTEYEF3Pp3dX6g3zV1d8QW449KBodxmRod8j%2BV3cI38xgAcGpKlzHGMpnLqzPfxOyKaMh11FU61PkdJmDGdWc0mOqCcG8SrnWiX1u7uneICsKSXDxGHS%2Fn9WI%2F80p2o1mVGhSLIYEWf90rLNhZmc4wq6aJ9gU67AElXiVuXceolqPC8GDSbK0YT%2FlxWPr1%2BAWw6GqP0jW9rwA%2B0BZwGTgmG7dYBtAU0E4JyKxSo23taYF6fFpMr3AeErwLj%2BFR2VwuKtrG8ELMwBEuIleRBc6iyudFMKOHf6davWLp4Bxfw%2F8pT7EB4VUZHEx%2FwFrOboWn6%2BYCYBz4gHb7FvRm6cQOCPyZCXfYCon%2BBwJJl%2FrCEDNvg9S2t6dMbS7zhBy8et6xNGgT%2BQKp%2FHIhiqM3zj6DfW6RVwu4gLK1MIPPechh4hFcsI5A6MHOvocT0fIYA%2FVWGFR%2Bw9ikh%2FKo0DzB7VO0t6nQzQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200518T102127Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZQXQOTH6Q%2F20200518%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=1a6d53c59d6219899502e5144492a32f0434908e6d6a018502def3849d9d6002 [following]\n",
      "--2020-05-18 03:21:33--  https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperEncoder-STEP-89000.pt?response-content-disposition=attachment%3B%20filename%3D%22JasperEncoder-STEP-89000.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJGMEQCIBk0tZOxMgN%2Bc6qMu3rqTITghZaj6pu6efQcbZ5KxeB7AiAPGZKlEHCSp7Wbh7i1W1seyVNNjrjSFvfQyMVX9DocFyq9AwjS%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDc4OTM2MzEzNTAyNyIM5FRiuDuzmFo9QSawKpEDZ6PeqYd0OC5B%2FzzCGgGEHwyuPgg%2B5WifOHqvRFFIb5b8Nl0RiiVWQyktqF6V%2BUx8nG2TIhPqftMMBgrV7kOHwd853SIy21qij4QLqa8m0%2FWHHvjPRd0ZUE0FMcr%2BgkLtmmOC6XDa3hDPWLigccfJYgesuh7OX%2FDnoCq0P7JVw%2BrihE6okulpAzLlFenoDb7GbFztdph%2Buw2amD%2Bt2BZqCC2DqTsASFkd9DVITOITcfsKjDBz%2BVFVvRIQKl7MsVEqSxtXBTatr5ODzjJ1yiNVylUy3tlm%2F1oOKAo4xPJD7lrhAgHknTE%2Bd7xDy%2Bu1Cr54okMrh%2F9tMlPMbrscY4O1uyXQIU651tXaKkNLpYCcji3Ajmp0jFU0Wc74njLhO6570fvTEYEF3Pp3dX6g3zV1d8QW449KBodxmRod8j%2BV3cI38xgAcGpKlzHGMpnLqzPfxOyKaMh11FU61PkdJmDGdWc0mOqCcG8SrnWiX1u7uneICsKSXDxGHS%2Fn9WI%2F80p2o1mVGhSLIYEWf90rLNhZmc4wq6aJ9gU67AElXiVuXceolqPC8GDSbK0YT%2FlxWPr1%2BAWw6GqP0jW9rwA%2B0BZwGTgmG7dYBtAU0E4JyKxSo23taYF6fFpMr3AeErwLj%2BFR2VwuKtrG8ELMwBEuIleRBc6iyudFMKOHf6davWLp4Bxfw%2F8pT7EB4VUZHEx%2FwFrOboWn6%2BYCYBz4gHb7FvRm6cQOCPyZCXfYCon%2BBwJJl%2FrCEDNvg9S2t6dMbS7zhBy8et6xNGgT%2BQKp%2FHIhiqM3zj6DfW6RVwu4gLK1MIPPechh4hFcsI5A6MHOvocT0fIYA%2FVWGFR%2Bw9ikh%2FKo0DzB7VO0t6nQzQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200518T102127Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZQXQOTH6Q%2F20200518%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=1a6d53c59d6219899502e5144492a32f0434908e6d6a018502def3849d9d6002\n",
      "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.218.181.56\n",
      "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.218.181.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 311934 (305K) [application/octet-stream]\n",
      "Saving to: ‘./checkpoints/matchboxnet_v2-3x1x64/JasperEncoder-STEP-89000.pt’\n",
      "\n",
      "JasperEncoder-STEP- 100%[===================>] 304.62K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2020-05-18 03:21:33 (3.35 MB/s) - ‘./checkpoints/matchboxnet_v2-3x1x64/JasperEncoder-STEP-89000.pt’ saved [311934/311934]\n",
      "\n",
      "--2020-05-18 03:21:33--  https://api.ngc.nvidia.com/v2/models/nvidia/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperDecoderForClassification-STEP-89000.pt\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 54.241.185.37, 54.153.26.224\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|54.241.185.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperDecoderForClassification-STEP-89000.pt?response-content-disposition=attachment%3B%20filename%3D%22JasperDecoderForClassification-STEP-89000.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJGMEQCIHYBuunfnTP4XxlkFZjLKO6JMbkXSJDZu%2FmzFAxOt%2BCkAiBmWSdyY4kB8E715SH%2B7uq5LKj47ZrjxJkDDfGU73XwrSq9AwjS%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDc4OTM2MzEzNTAyNyIM3ovLYuUOlXS14eKIKpEDGG41Fki8ZqM9GzYjsp4K2yg%2BRZVVOtURQvr%2FirGr%2B0pKTAtdsDeqrCgQ%2FhpcvID0u5oIxuxNpRppg9NSfkqWvqJG9It%2BfLeoDE5IgeZInHZGvEGNkDjcVjthWMfcVA2Oh60ez51ymtvZI9vBzhH2VuAcF08PR01UtlWcpCXLBvu0o%2FAk%2B2Z1uA5enXbQFjyvY7FekEABRfVldgthLkHKQ5u44vi7X81fGwSuwVdCo5L2s1FP8UDZv89bYQAkP%2BGNuCQPJIJO2U5OUfPlWAke4UqdQ%2Fq1dXQ3YzheRqHchkiHBFcTHvZF38fEhQFTdDEaG0RLtTeIkHVcxcv8Oh%2FLRuuMDHzsK0X05vJxyimNQHXk1ZtP7fScAX79jZFXeQ6JWZusBYsVHTLHD05yBL4EuxWUz0gWP42RM1%2Ff%2BfxCmSVzpyVpbSke01uK0eTz6of5jR4uLliJ7h%2F3SwbXey8SlpRuaRDgdSReJxPO0vSOBODK7QLrSnZNKLsriv0gvEQGmcKUc0i9VRM0lylj6LSHJ3Aw3ZyJ9gU67AHtAm5Wt28ZfT1eta5n5%2BSpd5GSKTvp1YfmhJgJQ8Gcgqbw6G3dtAa3fe1H7H3CSNvxd2G8X7xCE11Yn6RfcTJcR9OAmV1%2BxlDlpkO0oPUMyI%2BPKnEvS9j%2Bq3tQ3vpNEssYs77lYdu864oWBjZqFuOyYXP4gCTa7hiyPrDS7FODIjI%2FQJkt%2Be3XWX0VA3vXNWdZBdsojg2TpJ3EBx1Xh%2F8BAOMpuQ7bAnj72JioQa1q9vYi7tqyCO0IVhr%2Fi9X6F1c%2Be6u9MQE%2FZF8RPexUdWs1CF%2F1FG77I9fiz3hQc9mh6ZOvtMTH9ThHpidEBQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200518T102128Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZRQQSJFR6%2F20200518%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=6190f3692056abde7ed100ec6b180b4facca270ff7e60d514d553847e6a492fd [following]\n",
      "--2020-05-18 03:21:33--  https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperDecoderForClassification-STEP-89000.pt?response-content-disposition=attachment%3B%20filename%3D%22JasperDecoderForClassification-STEP-89000.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMSJGMEQCIHYBuunfnTP4XxlkFZjLKO6JMbkXSJDZu%2FmzFAxOt%2BCkAiBmWSdyY4kB8E715SH%2B7uq5LKj47ZrjxJkDDfGU73XwrSq9AwjS%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAIaDDc4OTM2MzEzNTAyNyIM3ovLYuUOlXS14eKIKpEDGG41Fki8ZqM9GzYjsp4K2yg%2BRZVVOtURQvr%2FirGr%2B0pKTAtdsDeqrCgQ%2FhpcvID0u5oIxuxNpRppg9NSfkqWvqJG9It%2BfLeoDE5IgeZInHZGvEGNkDjcVjthWMfcVA2Oh60ez51ymtvZI9vBzhH2VuAcF08PR01UtlWcpCXLBvu0o%2FAk%2B2Z1uA5enXbQFjyvY7FekEABRfVldgthLkHKQ5u44vi7X81fGwSuwVdCo5L2s1FP8UDZv89bYQAkP%2BGNuCQPJIJO2U5OUfPlWAke4UqdQ%2Fq1dXQ3YzheRqHchkiHBFcTHvZF38fEhQFTdDEaG0RLtTeIkHVcxcv8Oh%2FLRuuMDHzsK0X05vJxyimNQHXk1ZtP7fScAX79jZFXeQ6JWZusBYsVHTLHD05yBL4EuxWUz0gWP42RM1%2Ff%2BfxCmSVzpyVpbSke01uK0eTz6of5jR4uLliJ7h%2F3SwbXey8SlpRuaRDgdSReJxPO0vSOBODK7QLrSnZNKLsriv0gvEQGmcKUc0i9VRM0lylj6LSHJ3Aw3ZyJ9gU67AHtAm5Wt28ZfT1eta5n5%2BSpd5GSKTvp1YfmhJgJQ8Gcgqbw6G3dtAa3fe1H7H3CSNvxd2G8X7xCE11Yn6RfcTJcR9OAmV1%2BxlDlpkO0oPUMyI%2BPKnEvS9j%2Bq3tQ3vpNEssYs77lYdu864oWBjZqFuOyYXP4gCTa7hiyPrDS7FODIjI%2FQJkt%2Be3XWX0VA3vXNWdZBdsojg2TpJ3EBx1Xh%2F8BAOMpuQ7bAnj72JioQa1q9vYi7tqyCO0IVhr%2Fi9X6F1c%2Be6u9MQE%2FZF8RPexUdWs1CF%2F1FG77I9fiz3hQc9mh6ZOvtMTH9ThHpidEBQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200518T102128Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZRQQSJFR6%2F20200518%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=6190f3692056abde7ed100ec6b180b4facca270ff7e60d514d553847e6a492fd\n",
      "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.218.181.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.218.181.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18713 (18K) [application/octet-stream]\n",
      "Saving to: ‘./checkpoints/matchboxnet_v2-3x1x64/JasperDecoderForClassification-STEP-89000.pt’\n",
      "\n",
      "JasperDecoderForCla 100%[===================>]  18.27K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-05-18 03:21:33 (888 KB/s) - ‘./checkpoints/matchboxnet_v2-3x1x64/JasperDecoderForClassification-STEP-89000.pt’ saved [18713/18713]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the checkpoint files\n",
    "base_checkpoint_path = './checkpoints/matchboxnet_v2-3x1x64/'\n",
    "CHECKPOINT_ENCODER = os.path.join(base_checkpoint_path, 'JasperEncoder-STEP-89000.pt')\n",
    "CHECKPOINT_DECODER = os.path.join(base_checkpoint_path, 'JasperDecoderForClassification-STEP-89000.pt')\n",
    "\n",
    "if not os.path.exists(base_checkpoint_path):\n",
    "    os.makedirs(base_checkpoint_path)\n",
    "    \n",
    "if not os.path.exists(CHECKPOINT_ENCODER):\n",
    "    !wget https://api.ngc.nvidia.com/v2/models/nvidia/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperEncoder-STEP-89000.pt -P {base_checkpoint_path};\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DECODER):\n",
    "    !wget https://api.ngc.nvidia.com/v2/models/nvidia/google_speech_commands_v2___matchboxnet_3x1x1/versions/1/files/JasperDecoderForClassification-STEP-89000.pt -P {base_checkpoint_path};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Neural Modules and the eval graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(MODEL_YAML) as f:\n",
    "    model_definition = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    placement=nemo.core.DeviceType.GPU,\n",
    "    backend=nemo.core.Backend.PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Neural Module to iterate over audio\n",
    "\n",
    "Here we define a custom Neural Module which acts as an iterator over a stream of audio that is supplied to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.backends.pytorch.nm import DataLayerNM\n",
    "from nemo.core.neural_types import NeuralType, AudioSignal, LengthsType\n",
    "import torch\n",
    "\n",
    "# simple data layer to pass audio signal\n",
    "class AudioDataLayer(DataLayerNM):\n",
    "    @property\n",
    "    def output_ports(self):\n",
    "        return {\n",
    "            'audio_signal': NeuralType(('B', 'T'), AudioSignal(freq=self._sample_rate)),\n",
    "            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, sample_rate):\n",
    "        super().__init__()\n",
    "        self._sample_rate = sample_rate\n",
    "        self.output = True\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        self.output = False\n",
    "        return torch.as_tensor(self.signal, dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape, dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signal):\n",
    "        self.signal = np.reshape(signal.astype(np.float32)/32768., [1, -1])\n",
    "        self.signal_shape = np.expand_dims(self.signal.size, 0).astype(np.int64)\n",
    "        self.output = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def data_iterator(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Neural Modules\n",
    "\n",
    "We now instantiate the neural modules and the encoder and decoder, set the weights of these models with the downloaded pretrained weights and construct the DAG to evaluate MatchboxNet on audio streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate necessary neural modules\n",
    "data_layer = AudioDataLayer(sample_rate=model_definition['sample_rate'])\n",
    "\n",
    "data_preprocessor = nemo_asr.AudioToMFCCPreprocessor(\n",
    "    **model_definition['AudioToMFCCPreprocessor'])\n",
    "\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    **model_definition['JasperEncoder'])\n",
    "\n",
    "jasper_decoder = nemo_asr.JasperDecoderForClassification(\n",
    "    feat_in=model_definition['JasperEncoder']['jasper'][-1]['filters'],\n",
    "    num_classes=len(model_definition['labels']))\n",
    "\n",
    "# load pre-trained model\n",
    "jasper_encoder.restore_from(CHECKPOINT_ENCODER)\n",
    "jasper_decoder.restore_from(CHECKPOINT_DECODER)\n",
    "\n",
    "# Define inference DAG\n",
    "audio_signal, audio_signal_len = data_layer()\n",
    "processed_signal, processed_signal_len = data_preprocessor(\n",
    "    input_signal=audio_signal,\n",
    "    length=audio_signal_len)\n",
    "encoded, encoded_len = jasper_encoder(audio_signal=processed_signal,\n",
    "                                      length=processed_signal_len)\n",
    "log_probs = jasper_decoder(encoder_output=encoded)\n",
    "\n",
    "# inference method for audio signal (single instance)\n",
    "def infer_signal(self, signal):\n",
    "    data_layer.set_signal(signal)\n",
    "    tensors = self.infer([log_probs], verbose=False)\n",
    "    logits = tensors[0][0]\n",
    "    return logits\n",
    "\n",
    "neural_factory.infer_signal = infer_signal.__get__(neural_factory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrameASR: Helper class for streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for streaming frame-based ASR\n",
    "# 1) use reset() method to reset FrameASR's state\n",
    "# 2) call transcribe(frame) to do ASR on\n",
    "#    contiguous signal's frames\n",
    "class FrameASR:\n",
    "    \n",
    "    def __init__(self, neural_factory, model_definition,\n",
    "                 frame_len=2, frame_overlap=2.5, \n",
    "                 offset=10):\n",
    "        '''\n",
    "        Args:\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          offset: number of symbols to drop for smooth streaming\n",
    "        '''\n",
    "        self.vocab = list(model_definition['labels'])\n",
    "        self.vocab.append('_')\n",
    "        \n",
    "        self.sr = model_definition['sample_rate']\n",
    "        self.frame_len = frame_len\n",
    "        self.n_frame_len = int(frame_len * self.sr)\n",
    "        self.frame_overlap = frame_overlap\n",
    "        self.n_frame_overlap = int(frame_overlap * self.sr)\n",
    "        timestep_duration = model_definition['AudioToMFCCPreprocessor']['window_stride']\n",
    "        for block in model_definition['JasperEncoder']['jasper']:\n",
    "            timestep_duration *= block['stride'][0] ** block['repeat']\n",
    "        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len,\n",
    "                               dtype=np.float32)\n",
    "        self.offset = offset\n",
    "        self.reset()\n",
    "        \n",
    "    def _decode(self, frame, offset=0):\n",
    "        assert len(frame)==self.n_frame_len\n",
    "        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\n",
    "        self.buffer[-self.n_frame_len:] = frame\n",
    "        logits = neural_factory.infer_signal(self.buffer).to('cpu').numpy()[0]\n",
    "        decoded = self._greedy_decoder(\n",
    "            logits, \n",
    "            self.vocab\n",
    "        )\n",
    "        return decoded[:len(decoded)-offset]\n",
    "    \n",
    "    def transcribe(self, frame=None):\n",
    "        if frame is None:\n",
    "            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\n",
    "        if len(frame) < self.n_frame_len:\n",
    "            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], 'constant')\n",
    "        unmerged = self._decode(frame, self.offset)\n",
    "        \n",
    "        return unmerged\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset frame_history and decoder's state\n",
    "        '''\n",
    "        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\n",
    "        self.prev_char = ''\n",
    "\n",
    "    @staticmethod\n",
    "    def _greedy_decoder(logits, vocab):\n",
    "        s = ''\n",
    "        \n",
    "        if logits.shape[0]:\n",
    "            s += str(vocab[np.argmax(logits)]) + \"\\n\"\n",
    "            \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration of signal frame, seconds\n",
    "FRAME_LEN = 0.25\n",
    "# number of audio channels (expect mono signal)\n",
    "CHANNELS = 1\n",
    "# sample rate, Hz\n",
    "RATE = 16000\n",
    "\n",
    "CHUNK_SIZE = int(FRAME_LEN*RATE)\n",
    "asr = FrameASR(neural_factory, model_definition,\n",
    "               frame_len=FRAME_LEN, frame_overlap=2.0, \n",
    "               offset=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What classes can this model recognize?\n",
    "\n",
    "Before we begin inference on the actual audio stream, lets look at what are the classes this model was trained to recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual     wow        learn      backward   dog        \n",
      "two        left       happy      nine       go         \n",
      "up         bed        stop       one        zero       \n",
      "tree       seven      on         four       bird       \n",
      "right      eight      no         six        forward    \n",
      "house      marvin     sheila     five       off        \n",
      "three      down       cat        follow     yes        \n"
     ]
    }
   ],
   "source": [
    "labels = model_definition['labels']\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(5):\n",
    "        print('%-10s' % (labels[i * 5 + j]), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin listening to audio stream and perform inference using FrameASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio input devices:\n",
      "0 pulse\n",
      "1 default\n",
      "Please type input device ID:\n",
      "1\n",
      "Listening...\n",
      "tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n",
      "tree\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cfde1b563a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = pa.PyAudio()\n",
    "print('Available audio input devices:')\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    if dev.get('maxInputChannels'):\n",
    "        print(i, dev.get('name'))\n",
    "print('Please type input device ID:')\n",
    "dev_idx = int(input())\n",
    "\n",
    "empty_counter = 0\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global empty_counter\n",
    "    signal = np.frombuffer(in_data, dtype=np.int16)\n",
    "    text = asr.transcribe(signal)\n",
    "    if len(text):\n",
    "        print(text,end='')\n",
    "        empty_counter = 3\n",
    "    elif empty_counter > 0:\n",
    "        empty_counter -= 1\n",
    "        if empty_counter == 0:\n",
    "            print(' ',end='')\n",
    "    return (in_data, pa.paContinue)\n",
    "\n",
    "stream = p.open(format=pa.paInt16,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=dev_idx,\n",
    "                stream_callback=callback,\n",
    "                frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "print('Listening...')\n",
    "\n",
    "stream.start_stream()\n",
    "\n",
    "while stream.is_active():\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree\n",
      "tree\n"
     ]
    }
   ],
   "source": [
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
