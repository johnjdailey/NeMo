{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import nemo\n",
    "import copy\n",
    "import tqdm\n",
    "import timeit\n",
    "import shutil\n",
    "import pathlib\n",
    "import commons\n",
    "import attrdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from ruamel import yaml\n",
    "from Bio import pairwise2\n",
    "from nemo.collections import asr as nemo_asr\n",
    "from nemo.collections.asr.helpers import post_process_predictions, post_process_transcripts, word_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGC_MAP = '~/Downloads/librivox-train-all.json'\n",
    "LOCAL_MAP = [\n",
    "    '/home/stanislavv/data/nemo-librispeech/train_clean_100.json',\n",
    "    '/home/stanislavv/data/nemo-librispeech/train_clean_360.json',\n",
    "    '/home/stanislavv/data/nemo-librispeech/train_other_500.json',\n",
    "]\n",
    "AD_HOC_MAP = '/home/stanislavv/data/librispeech/train-all.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE = pathlib.Path('/home/stanislavv/data/librispeech')\n",
    "\n",
    "NGC_BASE = DATA_BASE / 'ngc'\n",
    "NGC = [\n",
    "    NGC_BASE / 'librivox-dev-clean.json',\n",
    "    NGC_BASE / 'librivox-dev-other.json',\n",
    "    NGC_BASE / 'librivox-test-clean.json',\n",
    "    NGC_BASE / 'librivox-test-other.json',\n",
    "    NGC_BASE / 'librivox-train-all.json',\n",
    "]\n",
    "\n",
    "LOCAL_BASE = DATA_BASE / 'local'\n",
    "LOCAL = [\n",
    "    LOCAL_BASE / 'dev_clean.json',\n",
    "    LOCAL_BASE / 'dev_other.json',\n",
    "    LOCAL_BASE / 'test_clean.json',\n",
    "    LOCAL_BASE / 'test_other.json',\n",
    "    LOCAL_BASE / 'train_all.json',\n",
    "]\n",
    "\n",
    "NEW_LOCAL_BASE = DATA_BASE / 'new-local'\n",
    "NEW_LOCAL = [\n",
    "    NEW_LOCAL_BASE / 'dev-clean.json',\n",
    "    NEW_LOCAL_BASE / 'dev-other.json',\n",
    "    NEW_LOCAL_BASE / 'test-clean.json',\n",
    "    NEW_LOCAL_BASE / 'test-other.json',\n",
    "    NEW_LOCAL_BASE / 'train-all.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngc_id(audio_file):\n",
    "    return os.path.basename(audio_file)[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(ngc_map, local_map, new_file):\n",
    "    order = [\n",
    "        get_ngc_id(example['audio_file']) \n",
    "        for example in nemo.collections.asr.parts.manifest.item_iter(str(ngc_map))\n",
    "    ]\n",
    "    \n",
    "    local_id_index = [\n",
    "        (get_ngc_id(example['audio_file']), i)\n",
    "        for i, example in enumerate(nemo.collections.asr.parts.manifest.item_iter(str(local_map)))\n",
    "    ]\n",
    "    \n",
    "    local_id_index_dict = dict(local_id_index)\n",
    "    new_order = [local_id_index_dict[id_] for id_ in order]\n",
    "\n",
    "    lines = []\n",
    "    with open(local_map, 'r') as f:\n",
    "        lines.extend(list(f))\n",
    "    lines = [lines[id_] for id_ in new_order]\n",
    "    \n",
    "    with open(new_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngc, local, new_local in zip(NGC, LOCAL, NEW_LOCAL):\n",
    "    remap(ngc, local, new_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = '/home/stanislavv/data/ljspeech/local/split3/test.json'\n",
    "SAMPLE_RATE = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = '/home/stanislavv/data/libritts/local-22k/train-all.json'\n",
    "SAMPLE_RATE = 22050  # FROM LJSPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-04-25 07:03:49 deprecated:68] Function ``_get_trainer`` is deprecated. It is going to be removed in the future version.\n"
     ]
    }
   ],
   "source": [
    "runner = nemo.core.NeuralModuleFactory(\n",
    "    placement=nemo.core.DeviceType.GPU,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\' \\', \\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\', \\'h\\', \\'i\\', \\'j\\', \\'k\\', \\'l\\', \\'m\\', \\'n\\', \\'o\\', \\'p\\', \\'q\\', \\'r\\', \\'s\\', \\'t\\', \\'u\\', \\'v\\', \\'w\\', \\'x\\', \\'y\\', \\'z\\', \"\\'\"]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/asr/configs/quartznet15x5.yaml'\n",
    "yaml_loader = yaml.YAML(typ=\"safe\")\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "config.sample_rate = SAMPLE_RATE\n",
    "labels = list(config.labels)\n",
    "str(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'dither': 0.0, 'features': 80, 'frame_splicing': 1, 'highfreq': 8000, 'log': True, 'log_zero_guard_type': 'clamp', 'log_zero_guard_value': 1e-05, 'lowfreq': 0, 'mag_power': 1.0, 'n_fft': 1024, 'n_window_size': 1024, 'n_window_stride': 256, 'normalize': None, 'pad_to': 16, 'pad_value': -11.52, 'preemph': None, 'sample_rate': 24000, 'stft_conv': True, 'window': 'hann', 'window_size': None, 'window_stride': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/tts/configs/fasterspeech.yaml'\n",
    "yaml_loader = yaml.YAML(typ=\"safe\")\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    pp_config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "pp_config.sample_rate = SAMPLE_RATE\n",
    "pp_config.AudioToMelSpectrogramPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dl_params = copy.deepcopy(config.AudioToTextDataLayer)\n",
    "eval_dl_params.update(config.AudioToTextDataLayer[\"eval\"])\n",
    "del eval_dl_params[\"train\"]\n",
    "del eval_dl_params[\"eval\"]\n",
    "eval_dl_params['shuffle'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'window_size': None, 'window_stride': None, 'n_window_size': 512, 'n_window_stride': 256, 'window': 'hann', 'normalize': 'per_feature', 'n_fft': 512, 'preemph': None, 'features': 64, 'lowfreq': 0, 'highfreq': None, 'log': True, 'log_zero_guard_type': 'clamp', 'log_zero_guard_value': 1e-05, 'dither': 0.0, 'pad_to': 16, 'frame_splicing': 1, 'stft_conv': True, 'pad_value': -11.52, 'mag_power': 2.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/asr/configs/quartznet15x5-libritts-durs.yaml'\n",
    "yaml_loader = yaml.YAML(typ='safe')\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "\n",
    "config.sample_rate = SAMPLE_RATE\n",
    "config.AudioToMelSpectrogramPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-25 07:12:11 collections:144] Dataset loaded with 300 files totalling 0.55 hours\n",
      "[NeMo I 2020-04-25 07:12:11 collections:145] 0 files were filtered totalling 0.00 hours\n"
     ]
    }
   ],
   "source": [
    "data_layer = nemo_asr.AudioToTextDataLayer(\n",
    "    manifest_filepath=MAP,\n",
    "    sample_rate=config.sample_rate,\n",
    "    labels=config.labels,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 221085]), torch.Size([64]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, al, t, tl = next(iter(data_layer._dataloader))\n",
    "a.shape, al.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-25 07:12:11 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-25 07:12:11 features:152] STFT using conv\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "    sample_rate=config.sample_rate, **config.AudioToMelSpectrogramPreprocessor\n",
    ")\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    feat_in=config.AudioToMelSpectrogramPreprocessor[\"features\"], **config.JasperEncoder\n",
    ")\n",
    "jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
    "    feat_in=config.JasperEncoder[\"jasper\"][-1][\"filters\"], num_classes=len(config.labels)\n",
    ")\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signal_e1, a_sig_length_e1, transcript_e1, transcript_len_e1 = data_layer()\n",
    "processed_signal_e1, p_length_e1 = data_preprocessor(input_signal=audio_signal_e1, length=a_sig_length_e1)\n",
    "encoded_e1, encoded_len_e1 = jasper_encoder(audio_signal=processed_signal_e1, length=p_length_e1)\n",
    "log_probs_e1 = jasper_decoder(encoder_output=encoded_e1)\n",
    "predictions_e1 = greedy_decoder(log_probs=log_probs_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-25 07:12:14 actions:1463] Restoring JasperEncoder from /home/stanislavv/data/ckpts/nemo-qn15x5-libritts_tts-config_300epochs/JasperEncoder-STEP-410400.pt\n",
      "[NeMo I 2020-04-25 07:12:14 actions:1463] Restoring JasperDecoderForCTC from /home/stanislavv/data/ckpts/nemo-qn15x5-libritts_tts-config_300epochs/JasperDecoderForCTC-STEP-410400.pt\n",
      "[NeMo I 2020-04-25 07:12:14 actions:737] Evaluating batch 0 out of 5\n",
      "[NeMo I 2020-04-25 07:12:14 actions:737] Evaluating batch 1 out of 5\n",
      "[NeMo I 2020-04-25 07:12:14 actions:737] Evaluating batch 2 out of 5\n",
      "[NeMo I 2020-04-25 07:12:15 actions:737] Evaluating batch 3 out of 5\n",
      "[NeMo I 2020-04-25 07:12:15 actions:737] Evaluating batch 4 out of 5\n"
     ]
    }
   ],
   "source": [
    "eval_tensors = [log_probs_e1, predictions_e1, transcript_e1, transcript_len_e1, encoded_len_e1, p_length_e1]\n",
    "load_dir = '/home/stanislavv/data/ckpts/nemo-qn15x5-libritts_tts-config_300epochs'\n",
    "evaluated_tensors = runner.infer(tensors=eval_tensors, checkpoint_dir=load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-18 05:53:25 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-18 05:53:25 features:152] STFT using conv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nemo.collections.asr.audio_preprocessing.AudioToMelSpectrogramPreprocessor at 0x7fa4c2546a58>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/tts/configs/fasterspeech-lj.yaml'\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    tts_config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "\n",
    "tts_pp = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "    **tts_config.AudioToMelSpectrogramPreprocessor\n",
    ")\n",
    "tts_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13036/13036 [00:21<00:00, 615.32it/s]\n"
     ]
    }
   ],
   "source": [
    "tts_lens = []\n",
    "for e in tqdm.tqdm(data_layer._dataset):\n",
    "    a, al = e[0], e[1]\n",
    "    _, ml = tts_pp.forward(a.unsqueeze_(0).cuda(), al.unsqueeze_(0).cuda())\n",
    "    tts_lens.append(ml.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 688, 461, 393, 857, 587, 655, 527, 448, 568]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_lens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 688, 461, 393, 857, 587, 655, 527, 448, 568]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_lens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_lens == asr_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16458752515090544"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = post_process_transcripts(evaluated_tensors[2], evaluated_tensors[3], config.labels)\n",
    "greedy_hypotheses = post_process_predictions(evaluated_tensors[1], config.labels)\n",
    "word_error_rate(greedy_hypotheses, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z , . ! ? ; : - / \" ' ( ) [ ] { }\n"
     ]
    }
   ],
   "source": [
    "labels = list(config.labels)\n",
    "print(*labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = evaluated_tensors[2][0]\n",
    "text_len = evaluated_tensors[3][0]\n",
    "ctc_tokens = evaluated_tensors[1][0]\n",
    "ctc_logprobs = evaluated_tensors[0][0]\n",
    "ctc_len = evaluated_tensors[4][0]\n",
    "mel_len = evaluated_tensors[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text[0].numpy()\n",
    "text_len1 = text_len[0].numpy().item()\n",
    "ctc_tokens1 = ctc_tokens[0].numpy()\n",
    "ctc_logprobs1 = ctc_logprobs[0].numpy()\n",
    "ctc_len1 = ctc_len[0].numpy().item()\n",
    "mel_len1 = mel_len[0].numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 642, (642, 70))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = list(text1[:text_len1])\n",
    "ctc_tokens1 = list(ctc_tokens1[:ctc_len1])\n",
    "ctc_logprobs1 = ctc_logprobs1[:ctc_len1]\n",
    "len(text1), len(ctc_tokens1), ctc_logprobs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  5, 12,  3,  4,  4,  7,  1,  7,  2,  7,  4,  1,  5,  4,  6,\n",
       "        0,  9,  5,  2,  2,  9,  5,  7,  8,  1, 12,  1,  2,  8,  2,  1,  1,\n",
       "        5, 20,  7,  5, 18,  8,  4,  7,  3,  3, 12,  1,  8,  5,  2,  3, 10,\n",
       "        2,  1, 17,  6,  4,  6, 16, 56,  2,  4,  2,  9,  1,  2,  5, 10,  8,\n",
       "        3,  1,  2, 28,  1,  2,  7,  4,  2,  6,  3,  7,  8,  3,  4,  2,  9,\n",
       "        2,  1,  5, 10,  3,  2,  9,  3,  9,  3,  3,  3,  2,  3, 13,  3,  5,\n",
       "        5, 10, 15, 18])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PadProcesser:\n",
    "    def __init__(self, labels):\n",
    "        labels = labels + ['~']\n",
    "        self.blank_id = len(labels) - 1\n",
    "        self.space_id = labels.index(' ')\n",
    "        self.labels_map = dict([(i, labels[i]) for i in range(len(labels))])\n",
    "    \n",
    "    def bound_text(self, tokens):\n",
    "        return [self.space_id] + tokens + [self.space_id]\n",
    "    \n",
    "    def bound_ctc(self, tokens, logprobs):\n",
    "        tokens = [self.space_id, self.blank_id] + tokens + [self.blank_id, self.space_id]\n",
    "        \n",
    "        logprobs = np.lib.pad(logprobs, ((2, 2), (0, 0)), 'edge')\n",
    "\n",
    "        def swap(col, a, b):\n",
    "            logprobs[col][a], logprobs[col][b] = logprobs[col][b], logprobs[col][a]\n",
    "        \n",
    "        first_token, last_token = tokens[2], tokens[-3]\n",
    "        swap(0, first_token, self.space_id)\n",
    "        swap(1, first_token, self.blank_id)\n",
    "        swap(-1, last_token, self.space_id)\n",
    "        swap(-2, last_token, self.blank_id)\n",
    "\n",
    "        return tokens, logprobs\n",
    "    \n",
    "    def merge(self, tokens):\n",
    "        output_tokens = []\n",
    "        output_cnts = []\n",
    "        cnt = 0\n",
    "        for i in range(len(tokens)):\n",
    "            if i != 0 and (tokens[i - 1] != tokens[i]):\n",
    "                output_tokens.append(tokens[i - 1])\n",
    "                output_cnts.append(cnt)\n",
    "\n",
    "                cnt = 0\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "        output_tokens.append(tokens[-1])\n",
    "        output_cnts.append(cnt)\n",
    "        \n",
    "        assert sum(output_cnts) == len(tokens), f'SUM_CHECK {sum(output_cnts)} vs {len(tokens)}'\n",
    "\n",
    "        return output_tokens, output_cnts\n",
    "    \n",
    "    def merge_with_blanks(self, tokens, cnts, logprobs=None):\n",
    "        def choose_sep(l, r, a, b):\n",
    "            # `tokens[l] == a and tokens[r] == b`.\n",
    "            sum_a, sum_b = logprobs[l, a], logprobs[l + 1:r + 1, b].sum()\n",
    "            best_sum, best_sep = sum_a + sum_b, 0\n",
    "            for sep in range(1, r - l):\n",
    "                sum_a += logprobs[l + sep, a]\n",
    "                sum_b -= logprobs[l + sep, b]\n",
    "                if sum_a + sum_b > best_sum:\n",
    "                    best_sum, best_sep = sum_a + sum_b, sep\n",
    "\n",
    "            return best_sep\n",
    "        \n",
    "        output_tokens = []\n",
    "        output_durs = []\n",
    "        blank_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for token, cnt in zip(tokens, cnts):\n",
    "            total_cnt += cnt\n",
    "            if token == self.blank_id:\n",
    "                blank_cnt += cnt\n",
    "                continue\n",
    "            \n",
    "            output_tokens.append(token)\n",
    "            \n",
    "            if logprobs is None:\n",
    "                # Half half.\n",
    "                left_cnt = blank_cnt // 2\n",
    "            else:\n",
    "                # Clever sep choice based on sum of log probs.\n",
    "                left_cnt = choose_sep(\n",
    "                    l=total_cnt - cnt - blank_cnt - 1,\n",
    "                    r=total_cnt - cnt,\n",
    "                    a=output_tokens[-1],\n",
    "                    b=token,\n",
    "                )\n",
    "            right_cnt = blank_cnt - left_cnt\n",
    "            blank_cnt = 0\n",
    "            \n",
    "            if left_cnt:\n",
    "                output_durs[-1] += left_cnt\n",
    "            output_durs.append(cnt + right_cnt)\n",
    "        \n",
    "        output_durs[-1] += blank_cnt\n",
    "\n",
    "        assert sum(output_durs) == sum(cnts), f'SUM_CHECK {sum(output_durs)} vs {sum(cnts)}'\n",
    "\n",
    "        return output_tokens, output_durs\n",
    "    \n",
    "    def align(self, output_tokens, gt_text):\n",
    "        def make_str(tokens):\n",
    "            return ''.join(self.labels_map[c] for c in tokens)\n",
    "        \n",
    "        s = make_str(output_tokens)\n",
    "        t = make_str(gt_text)\n",
    "        alignmet = pairwise2.align.globalxx(s, t, gap_char='%')[0]\n",
    "        sa, ta, *_ = alignmet\n",
    "        return sa, ta\n",
    "    \n",
    "    def generate(self, gt_text, alignment, durs):\n",
    "        output_tokens = []\n",
    "        output_cnts = []\n",
    "        si, ti = 0, 0\n",
    "#         print(len(durs))\n",
    "        assert len(alignment[0]) == len(alignment[1])\n",
    "        for sc, tc in zip(*alignment):\n",
    "#             print(si, sc, ti, tc)\n",
    "            if sc == '%' and tc == '%':\n",
    "                print('NO WAY')\n",
    "                continue\n",
    "            \n",
    "            if sc == '%':\n",
    "                output_tokens.append(self.blank_id)\n",
    "                output_cnts.append(durs[si])\n",
    "                si += 1\n",
    "            elif tc == '%':\n",
    "                output_tokens.append(gt_text[ti])\n",
    "                output_cnts.append(0)\n",
    "                ti += 1\n",
    "            else:\n",
    "                output_tokens.append(gt_text[ti])\n",
    "                output_cnts.append(durs[si])\n",
    "                si += 1\n",
    "                ti += 1\n",
    "\n",
    "        assert sum(output_cnts) == sum(durs)\n",
    "        \n",
    "        return output_tokens, output_cnts\n",
    "\n",
    "    def __call__(self, text, ctc_tokens, ctc_logprobs, mel_len):\n",
    "        # This adds +2 tokens.\n",
    "        text = self.bound_text(text)\n",
    "        # This add +4 tokens, 2 of them are blank.\n",
    "        ctc_tokens, ctc_logprobs = self.bound_ctc(ctc_tokens, ctc_logprobs)\n",
    "\n",
    "        ctc_tokens, ctc_cnts = self.merge(ctc_tokens)\n",
    "        ctc_tokens, ctc_durs = self.merge_with_blanks(ctc_tokens, ctc_cnts, ctc_logprobs)\n",
    "        \n",
    "        alignment = self.align(text, ctc_tokens)\n",
    "        tokens, cnts = self.generate(text, alignment, ctc_durs)\n",
    "        tokens, durs = self.merge_with_blanks(tokens, cnts)\n",
    "        assert tokens == text, 'EXACT_TOKENS_MATCH_CHECK'\n",
    "\n",
    "        def adjust(start, direction, value):\n",
    "            i = start\n",
    "            while value != 0:\n",
    "                dur = durs[i]\n",
    "                \n",
    "                if value < 0:\n",
    "                    durs[i] = dur - value\n",
    "                else:\n",
    "                    durs[i] = max(dur - value, 0)\n",
    "                \n",
    "                value -= dur - durs[i]\n",
    "                i += direction\n",
    "\n",
    "        adjust(0, 1, 4)\n",
    "        adjust(-1, -1, sum(durs) - mel_len)  # Including 4 suffix bound tokens.\n",
    "        assert durs[0] >= 0, f'{durs[0]}'\n",
    "        assert durs[-1] >= 0, f'{durs[-1]}'\n",
    "        \n",
    "        durs = np.array(durs, dtype=np.long)\n",
    "        assert durs.shape[0] == len(text), f'LEN_CHECK {durs.shape[0]} vs {len(text)}'\n",
    "        assert np.sum(durs) == mel_len, f'SUM_CHECK {np.sum(durs)} vs {mel_len}'\n",
    "\n",
    "        return durs\n",
    "\n",
    "processer = PadProcesser(labels)\n",
    "durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "durs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq:\n",
    "    def __init__(self, tokens, cnts=None):\n",
    "        if cnts is None:\n",
    "            cnts = np.ones(len(tokens), dtype=np.long)\n",
    "\n",
    "        assert len(tokens) == len(cnts)\n",
    "        self.tokens = tokens\n",
    "        self.cnts = cnts\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(list(zip(self.tokens, self.cnts)))\n",
    "    \n",
    "    @property\n",
    "    def total(self):\n",
    "        return sum(self.cnts)\n",
    "    \n",
    "    def merge(self):\n",
    "        output_tokens = []\n",
    "        output_cnts = []\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(len(self.tokens)):\n",
    "            if i != 0 and (self.tokens[i - 1] != self.tokens[i]):\n",
    "                output_tokens.append(self.tokens[i - 1])\n",
    "                output_cnts.append(cnt)\n",
    "\n",
    "                cnt = 0\n",
    "\n",
    "            cnt += self.cnts[i]\n",
    "\n",
    "        output_tokens.append(self.tokens[-1])\n",
    "        output_cnts.append(cnt)\n",
    "        \n",
    "        assert sum(output_cnts) == sum(self.cnts), \\\n",
    "            f'SUM-CHECK {sum(output_cnts)} vs {sum(self.cnts)}'\n",
    "\n",
    "        return Seq(output_tokens, output_cnts)\n",
    "    \n",
    "    def full_pad(self, blank_id, blank_cnt=1):\n",
    "        output_tokens = [blank_id]\n",
    "        output_cnts = [blank_cnt]\n",
    "\n",
    "        for token, cnt in zip(self.tokens, self.cnts):\n",
    "            output_tokens.append(token)\n",
    "            output_cnts.append(cnt)\n",
    "            \n",
    "            output_tokens.append(blank_id)\n",
    "            output_cnts.append(blank_cnt)\n",
    "        \n",
    "        return Seq(output_tokens, output_cnts)\n",
    "    \n",
    "    def adjust_cnt(self, value, start=-1, direction='left'):\n",
    "        tokens, cnts = self.tokens, self.cnts.copy()\n",
    "        \n",
    "        i, di = start, -1 if direction == 'left' else 1\n",
    "        while value != 0:\n",
    "            cnt = cnts[i]\n",
    "\n",
    "            if value < 0:\n",
    "                cnts[i] = cnt - value\n",
    "            else:\n",
    "                cnts[i] = max(cnt - value, 0)\n",
    "\n",
    "            value -= cnt - cnts[i]\n",
    "            i += di\n",
    "        \n",
    "        return Seq(tokens, cnts)\n",
    "    \n",
    "    def split2(self):\n",
    "        tokens1, cnts1 = [], []\n",
    "        tokens2, cnts2 = [], []\n",
    "        turn = 1\n",
    "        \n",
    "        for token, cnt in zip(self.tokens, self.cnts):\n",
    "            if turn == 1:\n",
    "                tokens1.append(token)\n",
    "                cnts1.append(cnt)\n",
    "            else:\n",
    "                tokens2.append(token)\n",
    "                cnts2.append(cnt)\n",
    "            \n",
    "            turn = 1 if turn == 2 else 2\n",
    "        \n",
    "        return Seq(tokens1, cnts1), Seq(tokens2, cnts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5,  4, 11,  1,  1,  5,  3,  2,  6,  1,  6,  3,  0,  3,  3,  4,  0,\n",
       "         8,  5,  1,  0,  7,  4,  2, 11,  0,  9,  2,  0,  6,  0,  0,  0,  4,\n",
       "        19,  6,  4, 17,  2,  8,  5,  1,  0,  5,  6,  0, 10,  2,  2,  7,  0,\n",
       "         0,  2, 19,  2,  6,  2, 67,  1,  3,  1,  6,  1,  1,  3,  9,  7,  2,\n",
       "         0,  0, 24,  3,  1,  5,  3,  1,  3,  1,  5,  6,  2,  2,  1,  6,  0,\n",
       "         0,  3,  8,  1,  2,  8,  2,  4,  5,  0,  0,  1,  0,  8,  6,  0,  3,\n",
       "        13, 29,  2]),\n",
       " array([1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 2,\n",
       "        1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 1,\n",
       "        1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "        1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 3, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2, 2,\n",
       "        1, 1, 1, 1, 1, 2, 3, 3, 1, 3, 1, 1, 2, 1, 1, 0]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FullProcessor(PadProcesser):\n",
    "    def __call__(self, text, ctc_tokens, ctc_logprobs, mel_len):\n",
    "        text = Seq(text).full_pad(self.blank_id)\n",
    "        ctc = Seq(ctc_tokens).merge().full_pad(self.blank_id, blank_cnt=0).merge()\n",
    "        \n",
    "        alignment = self.align(text.tokens, ctc.tokens)\n",
    "        gen = Seq(*self.generate(text.tokens, alignment, ctc.cnts)).merge()\n",
    "#         gen = gen.merge().adjust_cnt(gen.total - mel_len)\n",
    "        \n",
    "        # Two durs conditions.\n",
    "        assert gen.tokens == text.tokens\n",
    "        assert gen.total == mel_len\n",
    "        \n",
    "        blanks, text = gen.split2()\n",
    "        blanks = np.array(blanks.cnts, dtype=np.long)\n",
    "        cnts = np.array(text.cnts, dtype=np.long)\n",
    "        \n",
    "        assert len(blanks) == len(cnts) + 1\n",
    "        \n",
    "        return blanks, cnts\n",
    "\n",
    "\n",
    "processer = FullProcessor(labels)\n",
    "durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "durs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt(text):\n",
    "    print(*(ctc_labels[c] for c in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT:  a f t e r _ r e a d i n g _ i t , _ a n d _ f i n d i n g _ t h i s _ u n f a v o r a b l e _ t o _ h i m s e l f , _ h e _ r e s o l v e d _ t o _ c a r r y _ o u t _ h i s _ d e l i b e r a t e _ p l a n ,\n",
      "CTC:  ~ a ~ f ~ t ~ e ~ r ~ _ ~ r ~ e ~ a ~ d ~ i ~ n g ~ _ ~ i ~ t ~ _ ~ a ~ n d ~ _ ~ f ~ i ~ n d ~ i ~ n g ~ _ t h i ~ s ~ _ ~ u ~ n ~ f ~ a ~ v ~ o ~ r a ~ b ~ l e ~ _ ~ t ~ o ~ _ h i ~ m ~ s ~ e ~ l ~ f ~ , ~ _ ~ h ~ e ~ _ ~ r ~ e ~ s ~ o ~ l ~ v e d ~ _ ~ t ~ o ~ _ ~ c ~ a ~ r ~ r ~ y ~ _ ~ o ~ u ~ t ~ _ h i ~ s ~ _ ~ d ~ e ~ l ~ i ~ b ~ e r a ~ t e ~ _ ~ p l ~ a ~ n ~ . ~\n"
     ]
    }
   ],
   "source": [
    "ctc_labels = ['_'] + labels[1:] + ['~']\n",
    "print('GT: ', *(ctc_labels[c] for c in text1))\n",
    "print('CTC: ', *(ctc_labels[c] for c, _ in itertools.groupby(ctc_tokens1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = lambda: processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "# n, m = 10000, 280000\n",
    "# (timeit.timeit(f, number=n) / n) * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    text = batch[2].numpy()\n",
    "    text_len = batch[3].numpy()\n",
    "    ctc_tokens = batch[1].numpy()\n",
    "    ctc_logprobs = batch[0].numpy()\n",
    "    ctc_len = batch[4].numpy()\n",
    "    mel_len = batch[-1].numpy()\n",
    "\n",
    "    durs = []\n",
    "    for text1, text_len1, ctc_tokens1, ctc_logprobs1, ctc_len1, mel_len1 in zip(\n",
    "        text, text_len, ctc_tokens, ctc_logprobs, ctc_len, mel_len\n",
    "    ):\n",
    "        text1 = list(text1[:text_len1])\n",
    "        ctc_tokens1 = list(ctc_tokens1[:ctc_len1])\n",
    "        ctc_logprobs1 = ctc_logprobs1[:ctc_len1]\n",
    "        mel_len1 = mel_len1.item()\n",
    "        \n",
    "        assert mel_len1 == ctc_len1\n",
    "\n",
    "        durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "        \n",
    "        durs1, doable = commons.adjust_durs(*durs1)\n",
    "        if doable:\n",
    "            assert (durs1[-1] == 0).sum() == 0\n",
    "        \n",
    "        assert sum(durs1[0]) + sum(durs1[1]) == mel_len1\n",
    "        \n",
    "        durs.append(durs1)\n",
    "    \n",
    "    return durs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296aed4b633d4cd68297139834d0396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = tqdm.notebook.tqdm(zip(*evaluated_tensors), total=len(data_layer) // 64)\n",
    "batches = Parallel(n_jobs=16)(delayed(process_batch)(batch) for batch in it)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durs = []\n",
    "for batch in batches:\n",
    "    durs.extend(batch)\n",
    "\n",
    "len(durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354780"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "177390 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177390"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part0 = np.load('/home/stanislavv/data/librimeta/durs/libritts_300epochs-qn15x5-eqlen_all-1s_22k/train-all-0.npy', allow_pickle=True)\n",
    "len(part0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177390"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part1 = np.load('/home/stanislavv/data/librimeta/durs/libritts_300epochs-qn15x5-eqlen_all-1s_22k/train-all-1.npy', allow_pickle=True)\n",
    "len(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs = part0.tolist() + part1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354780"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durs = []\n",
    "for e in part0.tolist():\n",
    "    durs.append(tuple(e))\n",
    "for e in part1.tolist():\n",
    "    durs.append(tuple(e))\n",
    "len(durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6,  1,  4,  8,  1,  3,  4,  0,  2,  3, 11,  0,  3,  6,  6,  2,  0,\n",
       "        26,  3,  0,  0,  2,  4,  3,  0,  4,  0,  0,  1,  6,  1,  2,  5,  5,\n",
       "         1,  1,  2,  0,  2,  1, 10,  4,  0,  1,  0,  1,  6,  2,  2,  3,  7,\n",
       "         1, 14,  0,  7,  0,  0,  0,  5,  3,  2,  7,  2,  6,  2,  1,  4,  1,\n",
       "         2,  6,  8,  2,  0,  5,  1,  2,  4, 26,  1]),\n",
       " array([1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 2, 1, 3, 1, 1, 2, 2, 2,\n",
       "        2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 2,  0,  0,  2,  0,  4,  2,  2, 15,  3,  0, 11,  2,  4,  2,  1,  5,\n",
       "        0,  0,  0,  5,  1,  2,  7,  5,  6,  0,  0, 11,  5,  0,  2,  5,  7,\n",
       "        3,  1, 29,  2]),\n",
       "       array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1,\n",
       "       1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1])], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.load('/home/stanislavv/data/librimeta/durs/ljspeech_300epochs-qn15x5-eqlen_all-1s_split3/eval.npy', allow_pickle=True)\n",
    "t = np.load('/home/stanislavv/data/librimeta/durs/ljspeech_300epochs-qn15x5-eqlen_all-1s_split3/test.npy', allow_pickle=True)\n",
    "e[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 5,  4, 11,  1,  1,  5,  3,  2,  6,  1,  6,  3,  0,  3,  3,  4,  0,\n",
       "        7,  5,  1,  0,  7,  4,  2, 11,  0,  9,  2,  0,  6,  0,  0,  0,  4,\n",
       "       19,  6,  4, 17,  2,  8,  5,  1,  0,  5,  6,  0, 10,  2,  2,  7,  0,\n",
       "        0,  2, 19,  2,  6,  2, 67,  1,  3,  1,  6,  1,  1,  3,  9,  7,  2,\n",
       "        0,  0, 24,  3,  1,  5,  3,  1,  3,  1,  5,  6,  2,  2,  1,  6,  0,\n",
       "        0,  3,  8,  1,  2,  8,  2,  4,  5,  0,  0,  1,  0,  8,  6,  0,  3,\n",
       "       13, 28,  2]),\n",
       "       array([1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 1,\n",
       "       1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 3, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2, 2,\n",
       "       1, 1, 1, 1, 1, 2, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1])], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0088, 0.0149, 0.0185, 0.0210, 0.0230, 0.0246, 0.0259, 0.0271, 0.0282,\n",
       "        0.0291, 0.0299, 0.0307, 0.0314, 0.0320, 0.0327, 0.0332, 0.0338, 0.0343,\n",
       "        0.0347, 0.0352, 0.0356, 0.0360, 0.0364, 0.0368, 0.0372, 0.0375, 0.0378,\n",
       "        0.0381, 0.0385, 0.0388, 0.0390, 0.0393])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(32, dtype=torch.float)\n",
    "w = (w + 1).log() + 1\n",
    "w /= w.sum()\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs_dir = pathlib.Path('/home/stanislavv/data/librimeta/durs/ljspeech_300epochs-qn15x5-eqlen_all-1s_split3')\n",
    "name = f'test'\n",
    "durs_dir.mkdir(exist_ok=True)\n",
    "np.save(durs_dir / f'{name}.npy', durs, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a4f09383064daaa01922cf5d902925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2771.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-27efa7bf5368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmel_len1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mctc_len1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdurs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctc_tokens1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctc_logprobs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_len1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdurs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_durs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdurs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-0388ff4c704a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, ctc_tokens, ctc_logprobs, mel_len)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mctc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank_cnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0malignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         gen = gen.merge().adjust_cnt(gen.total - mel_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8c7964a34795>\u001b[0m in \u001b[0;36malign\u001b[0;34m(self, output_tokens, gt_text)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0malignmet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobalxx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_char\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignmet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stanislavv/Apps/Conda/envs/nemo/lib/python3.6/site-packages/Bio/pairwise2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **keywds)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;34m\"\"\"Call the alignment instance already created.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mkeywds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeywds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkeywds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stanislavv/Apps/Conda/envs/nemo/lib/python3.6/site-packages/Bio/pairwise2.py\u001b[0m in \u001b[0;36m_align\u001b[0;34m(sequenceA, sequenceB, match_fn, gap_A_fn, gap_B_fn, penalize_extend_when_opening, penalize_end_gaps, align_globally, gap_char, force_generic, score_only, one_alignment_only)\u001b[0m\n\u001b[1;32m    503\u001b[0m                                      \u001b[0mscore_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                                      \u001b[0malign_globally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_char\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                                      one_alignment_only, gap_A_fn, gap_B_fn)\n\u001b[0m\u001b[1;32m    506\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malignments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# This may happen, see recover_alignments for explanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stanislavv/Apps/Conda/envs/nemo/lib/python3.6/site-packages/Bio/pairwise2.py\u001b[0m in \u001b[0;36m_recover_alignments\u001b[0;34m(sequenceA, sequenceB, starts, best_score, score_matrix, trace_matrix, align_globally, gap_char, one_alignment_only, gap_A_fn, gap_B_fn, reverse)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdead_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mali_seqA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mali_seqB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0;31m# If trace is empty we have reached at least one border of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: This is going pretty fast.\n",
    "durs_dir = pathlib.Path('/home/stanislavv/data/librimeta/durs/libritts_300epochs-qn15x5-eqlen_all-1s_22k')\n",
    "name = f'train-all-1'\n",
    "durs_dir.mkdir(exist_ok=True)\n",
    "# k = -1\n",
    "\n",
    "durs = []\n",
    "for batch in tqdm.notebook.tqdm(zip(*evaluated_tensors), total=len(data_layer) // 64):\n",
    "    text = batch[2].numpy()\n",
    "    text_len = batch[3].numpy()\n",
    "    ctc_tokens = batch[1].numpy()\n",
    "    ctc_logprobs = batch[0].numpy()\n",
    "    ctc_len = batch[4].numpy()\n",
    "    mel_len = batch[-1].numpy()\n",
    "\n",
    "    for text1, text_len1, ctc_tokens1, ctc_logprobs1, ctc_len1, mel_len1 in zip(\n",
    "        text, text_len, ctc_tokens, ctc_logprobs, ctc_len, mel_len\n",
    "    ):\n",
    "        text1 = list(text1[:text_len1])\n",
    "        ctc_tokens1 = list(ctc_tokens1[:ctc_len1])\n",
    "        ctc_logprobs1 = ctc_logprobs1[:ctc_len1]\n",
    "        mel_len1 = mel_len1.item()\n",
    "        \n",
    "        assert mel_len1 == ctc_len1\n",
    "\n",
    "        durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "        \n",
    "        durs1 = commons.adjust_durs(*durs1)\n",
    "        assert (durs1[-1] == 0).sum() == 0\n",
    "        \n",
    "        assert sum(durs1[0]) + sum(durs1[1]) == mel_len1\n",
    "        \n",
    "        durs.append(durs1)\n",
    "\n",
    "\n",
    "np.save(durs_dir / f'{name}.npy', durs, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s o _ j o e _ w a s _ w h i p p e d _ b e c a u s e _ h e _ a l l o w e d _ t h e _ t u r k e y _ t o _ g e t _ i n t o _ t h e _ p o t .\n"
     ]
    }
   ],
   "source": [
    "tt(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(b) + sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6,  0,  0,  0,  0,  0,  5,  0,  0,  0,  3,  2,  3,  0,  5,  1,  2,\n",
       "         5,  0,  1, 18,  2,  1,  1,  8, 41,  4,  6,  1,  2,  1, 12,  2,  4,\n",
       "         6,  1,  0,  1,  3,  1,  2,  9,  2,  0,  2,  5,  2,  9,  4,  0,  3,\n",
       "         9,  2,  2, 17,  4,  0,  0,  2,  4,  0,  3,  2,  8,  0,  0,  2,  0,\n",
       "         0,  9,  1,  0,  0,  6,  3,  1,  3,  3,  2,  2,  4,  1,  2,  7,  0,\n",
       "         2, 34,  5,  2,  4,  0,  7,  3,  0,  2,  3,  0, 12,  3,  0,  8,  5,\n",
       "         8,  0, 13,  2,  4,  2,  3, 70,  5,  2,  6,  0,  8,  2,  6,  2,  0,\n",
       "         5,  0,  0,  0,  8,  4,  7,  2,  5,  1,  2,  6,  0,  0,  0,  4,  5,\n",
       "         4,  3,  7,  2,  0,  0, 36,  5,  1,  0,  3,  2,  0,  0,  6,  2,  3,\n",
       "         9,  3,  0,  2,  2, 12,  4,  0,  1,  5,  2, 14,  4,  2, 11,  4,  2,\n",
       "         5,  7,  1,  4,  2,  0,  4,  3,  1,  0, 11,  2,  2,  4,  4,  1,  2,\n",
       "        12,  4,  2,  3,  1,  2,  8,  5,  8,  0, 13,  3,  2,  8,  6,  2,  5,\n",
       "         1,  1,  2,  2,  0,  1,  2,  0,  5,  2,  2,  8,  0,  0,  0,  7,  1,\n",
       "         2,  4,  0,  2,  9,  4,  1,  8,  0, 34]),\n",
       " array([2, 3, 2, 3, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "        1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "        2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
       "        1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1,\n",
       "        2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1,\n",
       "        2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
       "        1, 1, 2, 1, 1, 2, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 69)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, d = durs1\n",
    "sum(b), len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6,  0,  0,  0,  0,  0,  5,  0,  0,  0,  3,  2,  3,  0,  5,  1,  2,\n",
       "         5,  0,  1, 18,  2,  1,  1,  8, 41,  4,  6,  1,  2,  1, 12,  2,  4,\n",
       "         6,  1,  0,  1,  3,  1,  2,  9,  2,  0,  2,  5,  2,  9,  4,  0,  3,\n",
       "         9,  2,  2, 17,  4,  0,  0,  2,  4,  0,  3,  2,  8,  0,  0,  2,  0,\n",
       "         0,  9,  1,  0,  0,  6,  3,  1,  3,  3,  2,  2,  4,  1,  2,  7,  0,\n",
       "         2, 34,  5,  2,  4,  0,  7,  3,  0,  2,  3,  0, 12,  3,  0,  8,  5,\n",
       "         8,  0, 13,  2,  4,  2,  3, 70,  5,  2,  6,  0,  8,  2,  6,  2,  0,\n",
       "         5,  0,  0,  0,  8,  4,  7,  2,  5,  1,  2,  6,  0,  0,  0,  4,  5,\n",
       "         4,  3,  7,  2,  0,  0, 36,  5,  1,  0,  3,  2,  0,  0,  6,  2,  3,\n",
       "         9,  3,  0,  2,  2, 12,  4,  0,  1,  5,  2, 14,  4,  2, 11,  4,  2,\n",
       "         5,  7,  1,  4,  2,  0,  4,  3,  1,  0, 11,  2,  2,  4,  4,  1,  2,\n",
       "        12,  4,  2,  3,  1,  2,  8,  5,  8,  0, 13,  3,  2,  8,  6,  2,  5,\n",
       "         1,  1,  2,  2,  0,  1,  2,  0,  5,  2,  2,  8,  0,  0,  0,  7,  1,\n",
       "         2,  4,  0,  2,  9,  4,  1,  8,  0, 34]),\n",
       " array([2, 3, 2, 3, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "        1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "        2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
       "        1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1,\n",
       "        2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1,\n",
       "        2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
       "        1, 1, 2, 1, 1, 2, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjust_durs(*durs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,   15,   16,   17,   18,   19,   20,   21,   22,   23,   24,\n",
       "         25,   26,   27,   28,   29,   30,   31,   32,   33,   34,   35,\n",
       "         36,   37,   38,   39,   40,   41,   42,   43,   44,   45,   46,\n",
       "         47,   48,   49,   50,   51,   52,   53,   54,   55,   56,   57,\n",
       "         58,   59,   60,   61,   62,   63,   64,   65,   66,   67,   68,\n",
       "         69,   70,   71,   72,   73,   74,   75,   76,   77,   78,   79,\n",
       "         80,   81,   82,   83,   84,   85,   86,   87,   88,   89,   90,\n",
       "         91,   92,   93,   94,   95,   96,   97,   98,   99,  100,  101,\n",
       "        102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  112,\n",
       "        113,  114,  116,  117,  118,  120,  122,  124,  126,  128,  130,\n",
       "        131,  132,  134,  136,  138,  140,  142,  144,  146,  148,  150,\n",
       "        152,  154,  156,  158,  160,  162,  164,  166,  168,  170,  172,\n",
       "        174,  176,  178,  180,  182,  184,  186,  188,  190,  192,  194,\n",
       "        196,  198,  200,  202,  204,  206,  208,  210,  212,  214,  216,\n",
       "        218,  220,  222,  224,  226,  228,  230,  232,  234,  236,  238,\n",
       "        240,  242,  244,  246,  248,  250,  252,  254,  256,  258,  260,\n",
       "        262,  264,  266,  268,  270,  272,  274,  276,  278,  280,  282,\n",
       "        284,  286,  288,  290,  292,  294,  296,  298,  300,  302,  304,\n",
       "        306,  308,  310,  312,  314,  316,  318,  320,  322,  324,  326,\n",
       "        328,  330,  332,  334,  336,  338,  340,  342,  344,  346,  348,\n",
       "        350,  352,  354,  356,  358,  360,  362,  364,  366,  368,  370,\n",
       "        372,  374,  376,  378,  380,  382,  384,  386,  388,  390,  392,\n",
       "        394,  396,  398,  400,  402,  404,  406,  408,  410,  412,  414,\n",
       "        416,  418,  420,  422,  424,  426,  428,  430,  432,  436,  438,\n",
       "        440,  442,  446,  448,  452,  454,  456,  458,  460,  462,  464,\n",
       "        466,  470,  472,  474,  476,  482,  484,  488,  490,  492,  494,\n",
       "        502,  508,  510,  512,  514,  518,  522,  524,  526,  534,  538,\n",
       "        540,  554,  558,  568,  586,  590,  592,  602,  630,  644,  652,\n",
       "        666,  670,  676,  684,  760,  816,  840, 1130])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(max_blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(b.max() for b, _ in durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(d.max() for _, d in durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1167549, 49408497, 2.363053059476794)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num, total = 0, 0\n",
    "for b, _ in durs:\n",
    "    num += (b > 30).sum()\n",
    "    total += len(b)\n",
    "\n",
    "num, total, (num / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31619, 49129870, 0.06435799646935765)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num, total = 0, 0\n",
    "for _, d in durs:\n",
    "    num += (d > 15).sum()\n",
    "    total += len(d)\n",
    "\n",
    "num, total, (num / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/278627 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 14127/278627 [00:00<00:01, 141258.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 26700/278627 [00:00<00:01, 136209.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 36747/278627 [00:00<00:01, 123074.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 49495/278627 [00:00<00:01, 124361.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 62138/278627 [00:00<00:01, 124971.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 74744/278627 [00:00<00:01, 125295.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 87506/278627 [00:00<00:01, 125982.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 100505/278627 [00:00<00:01, 127157.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 113237/278627 [00:00<00:01, 127205.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 125793/278627 [00:01<00:01, 126704.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 138547/278627 [00:01<00:01, 126951.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 151085/278627 [00:01<00:01, 126474.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 163550/278627 [00:01<00:00, 125234.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 175949/278627 [00:01<00:00, 123761.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 188242/278627 [00:01<00:00, 122351.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 200423/278627 [00:01<00:00, 121550.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 212541/278627 [00:01<00:00, 120701.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 224767/278627 [00:01<00:00, 121163.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 237060/278627 [00:01<00:00, 121684.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 249378/278627 [00:02<00:00, 122127.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 261584/278627 [00:02<00:00, 121933.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 278627/278627 [00:02<00:00, 123477.48it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(\n",
    "    list(itertools.chain.from_iterable(b for b, _ in tqdm.tqdm(durs))),\n",
    "    bins=range(20),\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
