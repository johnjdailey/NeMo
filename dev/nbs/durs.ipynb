{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import nemo\n",
    "import copy\n",
    "import tqdm\n",
    "import timeit\n",
    "import shutil\n",
    "import pathlib\n",
    "import commons\n",
    "import attrdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from ruamel import yaml\n",
    "from Bio import pairwise2\n",
    "from nemo.collections import asr as nemo_asr\n",
    "from nemo.collections.asr.helpers import post_process_predictions, post_process_transcripts, word_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGC_MAP = '~/Downloads/librivox-train-all.json'\n",
    "LOCAL_MAP = [\n",
    "    '/home/stanislavv/data/nemo-librispeech/train_clean_100.json',\n",
    "    '/home/stanislavv/data/nemo-librispeech/train_clean_360.json',\n",
    "    '/home/stanislavv/data/nemo-librispeech/train_other_500.json',\n",
    "]\n",
    "AD_HOC_MAP = '/home/stanislavv/data/librispeech/train-all.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE = pathlib.Path('/home/stanislavv/data/librispeech')\n",
    "\n",
    "NGC_BASE = DATA_BASE / 'ngc'\n",
    "NGC = [\n",
    "    NGC_BASE / 'librivox-dev-clean.json',\n",
    "    NGC_BASE / 'librivox-dev-other.json',\n",
    "    NGC_BASE / 'librivox-test-clean.json',\n",
    "    NGC_BASE / 'librivox-test-other.json',\n",
    "    NGC_BASE / 'librivox-train-all.json',\n",
    "]\n",
    "\n",
    "LOCAL_BASE = DATA_BASE / 'local'\n",
    "LOCAL = [\n",
    "    LOCAL_BASE / 'dev_clean.json',\n",
    "    LOCAL_BASE / 'dev_other.json',\n",
    "    LOCAL_BASE / 'test_clean.json',\n",
    "    LOCAL_BASE / 'test_other.json',\n",
    "    LOCAL_BASE / 'train_all.json',\n",
    "]\n",
    "\n",
    "NEW_LOCAL_BASE = DATA_BASE / 'new-local'\n",
    "NEW_LOCAL = [\n",
    "    NEW_LOCAL_BASE / 'dev-clean.json',\n",
    "    NEW_LOCAL_BASE / 'dev-other.json',\n",
    "    NEW_LOCAL_BASE / 'test-clean.json',\n",
    "    NEW_LOCAL_BASE / 'test-other.json',\n",
    "    NEW_LOCAL_BASE / 'train-all.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngc_id(audio_file):\n",
    "    return os.path.basename(audio_file)[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(ngc_map, local_map, new_file):\n",
    "    order = [\n",
    "        get_ngc_id(example['audio_file']) \n",
    "        for example in nemo.collections.asr.parts.manifest.item_iter(str(ngc_map))\n",
    "    ]\n",
    "    \n",
    "    local_id_index = [\n",
    "        (get_ngc_id(example['audio_file']), i)\n",
    "        for i, example in enumerate(nemo.collections.asr.parts.manifest.item_iter(str(local_map)))\n",
    "    ]\n",
    "    \n",
    "    local_id_index_dict = dict(local_id_index)\n",
    "    new_order = [local_id_index_dict[id_] for id_ in order]\n",
    "\n",
    "    lines = []\n",
    "    with open(local_map, 'r') as f:\n",
    "        lines.extend(list(f))\n",
    "    lines = [lines[id_] for id_ in new_order]\n",
    "    \n",
    "    with open(new_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngc, local, new_local in zip(NGC, LOCAL, NEW_LOCAL):\n",
    "    remap(ngc, local, new_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = '/home/stanislavv/data/ljspeech/local/split3/test.json'\n",
    "SAMPLE_RATE = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = '/home/stanislavv/data/libritts/local-22k/train-all.json'\n",
    "SAMPLE_RATE = 22050  # FROM LJSPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = nemo.core.NeuralModuleFactory(\n",
    "    placement=nemo.core.DeviceType.GPU,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a b c d e f g h i j k l m n o p q r s t u v w x y z '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttrDict({'window_size': 0.02, 'window_stride': None, 'window': 'hann', 'normalize': 'per_feature', 'n_fft': 512, 'features': 64, 'dither': 1e-05, 'pad_to': 16, 'stft_conv': True, 'n_window_stride': 256})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/asr/configs/quartznet15x5.yaml'\n",
    "yaml_loader = yaml.YAML(typ=\"safe\")\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "config.sample_rate = SAMPLE_RATE\n",
    "# config['AudioToMelSpectrogramPreprocessor']['window_size'] = 1024\n",
    "config['AudioToMelSpectrogramPreprocessor']['window_stride'] = None\n",
    "config['AudioToMelSpectrogramPreprocessor']['n_window_stride'] = 256\n",
    "# config['AudioToMelSpectrogramPreprocessor']['n_fft'] = 1024\n",
    "labels = list(config.labels)\n",
    "print(*labels)\n",
    "config.AudioToMelSpectrogramPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttrDict({'dither': 0.0, 'features': 80, 'frame_splicing': 1, 'highfreq': 8000, 'log': True, 'log_zero_guard_type': 'clamp', 'log_zero_guard_value': 1e-05, 'lowfreq': 0, 'mag_power': 1.0, 'n_fft': 1024, 'n_window_size': 1024, 'n_window_stride': 256, 'normalize': None, 'pad_to': 16, 'pad_value': -11.52, 'preemph': None, 'sample_rate': 24000, 'stft_conv': True, 'window': 'hann', 'window_size': None, 'window_stride': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/tts/configs/fasterspeech.yaml'\n",
    "yaml_loader = yaml.YAML(typ=\"safe\")\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    pp_config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "pp_config.sample_rate = SAMPLE_RATE\n",
    "pp_config.AudioToMelSpectrogramPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a b c d e f g h i j k l m n o p q r s t u v w x y z , . ! ? ; : - / \" ' ( ) [ ] { }\n"
     ]
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/tts/configs/fasterspeech-mels-lj.yaml'\n",
    "yaml_loader = yaml.YAML(typ='safe')\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    gconfig = attrdict.AttrDict(yaml_loader.load(f))\n",
    "\n",
    "gconfig.sample_rate = SAMPLE_RATE\n",
    "print(*gconfig.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-28 17:40:21 collections:144] Dataset loaded with 300 files totalling 0.55 hours\n",
      "[NeMo I 2020-04-28 17:40:21 collections:145] 0 files were filtered totalling 0.00 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 221085]), torch.Size([64]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_layer = nemo_asr.AudioToTextDataLayer(\n",
    "    manifest_filepath=MAP,\n",
    "    sample_rate=config.sample_rate,\n",
    "    labels=config.labels,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "a, al, t, tl = next(iter(data_layer._dataloader))\n",
    "a.shape, al.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-28 17:40:22 collections:144] Dataset loaded with 300 files totalling 0.55 hours\n",
      "[NeMo I 2020-04-28 17:40:22 collections:145] 0 files were filtered totalling 0.00 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([ 1,  6, 20,  5, 18,  0, 18,  5,  1,  4,  9, 14,  7,  0,  9, 20, 27,  0,\n",
       "          1, 14,  4,  0,  6,  9, 14,  4,  9, 14,  7,  0, 20,  8,  9, 19,  0, 21,\n",
       "         14,  6,  1, 22, 15, 18,  1,  2, 12,  5,  0, 20, 15,  0,  8,  9, 13, 19,\n",
       "          5, 12,  6, 27,  0,  8,  5,  0, 18,  5, 19, 15, 12, 22,  5,  4,  0, 20,\n",
       "         15,  0,  3,  1, 18, 18, 25,  0, 15, 21, 20,  0,  8,  9, 19,  0,  4,  5,\n",
       "         12,  9,  2,  5, 18,  1, 20,  5,  0, 16, 12,  1, 14, 27]),\n",
       " tensor([ 9, 20,  0,  9, 19,  0,  3, 15, 14,  3,  5,  9, 22,  1,  2, 12,  5,  0,\n",
       "         20,  8,  1, 20,  0, 20,  8, 15, 19,  5,  0, 13,  5, 14,  0, 23,  8, 15,\n",
       "          0,  8,  1,  4,  0, 12,  9, 20, 20, 12,  5,  0, 19, 12,  5,  5, 16, 27,\n",
       "          0,  1, 14,  4,  0, 23,  8, 15,  0,  8,  1,  4,  0,  3, 15, 14, 19, 21,\n",
       "         13,  5,  4,  0,  1, 12,  3, 15,  8, 15, 12,  9,  3,  0,  2,  5, 22,  5,\n",
       "         18,  1,  7,  5, 19, 27,  0,  5, 22,  5, 14,  0,  9, 14,  0, 12,  9, 13,\n",
       "          9, 20,  5,  4,  0, 17, 21,  1, 14, 20,  9, 20,  9,  5, 19, 27]),\n",
       " tensor([ 1, 14,  4,  0, 20,  8,  5, 18,  5,  0, 15, 14,  0, 10, 21, 14,  5,  0,\n",
       "         20,  8,  9, 18, 20,  5,  5, 14, 27,  0, 20,  8, 18,  5,  5,  0, 20, 23,\n",
       "          5, 14, 20, 25, 33, 20,  8, 18,  5,  5,  0,  2, 28,  3, 28, 27,  0,  8,\n",
       "          5,  0, 13,  5, 20,  0,  8,  9, 19,  0,  4,  5,  1, 20,  8, 28])]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full texts\n",
    "ft = nemo_asr.AudioToTextDataLayer(\n",
    "    manifest_filepath=MAP,\n",
    "    sample_rate=config.sample_rate,\n",
    "    labels=gconfig.labels,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "FT = [e[2] for e in ft._dataset]\n",
    "FT[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-28 17:40:24 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-28 17:40:24 features:152] STFT using conv\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "    sample_rate=config.sample_rate, **config.AudioToMelSpectrogramPreprocessor\n",
    ")\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    feat_in=config.AudioToMelSpectrogramPreprocessor[\"features\"], **config.JasperEncoder\n",
    ")\n",
    "jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
    "    feat_in=config.JasperEncoder[\"jasper\"][-1][\"filters\"], num_classes=len(config.labels)\n",
    ")\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signal_e1, a_sig_length_e1, transcript_e1, transcript_len_e1 = data_layer()\n",
    "processed_signal_e1, p_length_e1 = data_preprocessor(input_signal=audio_signal_e1, length=a_sig_length_e1)\n",
    "encoded_e1, encoded_len_e1 = jasper_encoder(audio_signal=processed_signal_e1, length=p_length_e1)\n",
    "log_probs_e1 = jasper_decoder(encoder_output=encoded_e1)\n",
    "predictions_e1 = greedy_decoder(log_probs=log_probs_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-28 17:40:25 actions:1468] Restoring JasperEncoder from /home/stanislavv/data/ckpts/nemo-quartznet-15x5/JasperEncoder-STEP-406556.pt\n",
      "[NeMo I 2020-04-28 17:40:25 actions:1468] Restoring JasperDecoderForCTC from /home/stanislavv/data/ckpts/nemo-quartznet-15x5/JasperDecoderForCTC-STEP-406556.pt\n",
      "[NeMo I 2020-04-28 17:40:25 actions:742] Evaluating batch 0 out of 5\n",
      "[NeMo I 2020-04-28 17:40:25 actions:742] Evaluating batch 1 out of 5\n",
      "[NeMo I 2020-04-28 17:40:26 actions:742] Evaluating batch 2 out of 5\n",
      "[NeMo I 2020-04-28 17:40:26 actions:742] Evaluating batch 3 out of 5\n",
      "[NeMo I 2020-04-28 17:40:26 actions:742] Evaluating batch 4 out of 5\n"
     ]
    }
   ],
   "source": [
    "eval_tensors = [log_probs_e1, predictions_e1, transcript_e1, transcript_len_e1, encoded_len_e1, processed_signal_e1, p_length_e1]\n",
    "# load_dir = '/home/stanislavv/data/ckpts/nemo-qn15x5-libritts_tts-config_300epochs'\n",
    "load_dir = '/home/stanislavv/data/ckpts/nemo-quartznet-15x5'\n",
    "evaluated_tensors = runner.infer(tensors=eval_tensors, checkpoint_dir=load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-04-18 05:53:25 features:144] PADDING: 16\n",
      "[NeMo I 2020-04-18 05:53:25 features:152] STFT using conv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nemo.collections.asr.audio_preprocessing.AudioToMelSpectrogramPreprocessor at 0x7fa4c2546a58>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = '/home/stanislavv/src/tts/NeMo/examples/tts/configs/fasterspeech-lj.yaml'\n",
    "with open(MODEL_CONFIG) as f:\n",
    "    tts_config = attrdict.AttrDict(yaml_loader.load(f))\n",
    "\n",
    "tts_pp = nemo_asr.AudioToMelSpectrogramPreprocessor(\n",
    "    **tts_config.AudioToMelSpectrogramPreprocessor\n",
    ")\n",
    "tts_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13036/13036 [00:21<00:00, 615.32it/s]\n"
     ]
    }
   ],
   "source": [
    "tts_lens = []\n",
    "for e in tqdm.tqdm(data_layer._dataset):\n",
    "    a, al = e[0], e[1]\n",
    "    _, ml = tts_pp.forward(a.unsqueeze_(0).cuda(), al.unsqueeze_(0).cuda())\n",
    "    tts_lens.append(ml.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 688, 461, 393, 857, 587, 655, 527, 448, 568]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_lens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 688, 461, 393, 857, 587, 655, 527, 448, 568]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_lens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_lens == asr_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060221870047543584"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = post_process_transcripts(evaluated_tensors[2], evaluated_tensors[3], config.labels)\n",
    "greedy_hypotheses = post_process_predictions(evaluated_tensors[1], config.labels)\n",
    "word_error_rate(greedy_hypotheses, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  a b c d e f g h i j k l m n o p q r s t u v w x y z '\n"
     ]
    }
   ],
   "source": [
    "labels = list(config.labels)\n",
    "print(*labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = evaluated_tensors[2][0]\n",
    "text_len = evaluated_tensors[3][0]\n",
    "ctc_tokens = evaluated_tensors[1][0]\n",
    "ctc_logprobs = evaluated_tensors[0][0]\n",
    "ctc_len = evaluated_tensors[4][0]\n",
    "mel_len = evaluated_tensors[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text[0].numpy()\n",
    "text_len1 = text_len[0].numpy().item()\n",
    "ctc_tokens1 = ctc_tokens[0].numpy()\n",
    "ctc_logprobs1 = ctc_logprobs[0].numpy()\n",
    "ctc_len1 = ctc_len[0].numpy().item()\n",
    "mel_len1 = mel_len[0].numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 96, (96, 29))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = list(text1[:text_len1])\n",
    "ctc_tokens1 = list(ctc_tokens1[:ctc_len1])\n",
    "ctc_logprobs1 = ctc_logprobs1[:ctc_len1]\n",
    "len(text1), len(ctc_tokens1), ctc_logprobs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 192)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * ctc_len1, mel_len1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([192, 828, 560, 844, 754, 622, 782, 692, 726, 362, 606, 644, 644, 160,\n",
       "        394, 678, 696, 376, 828, 258, 126, 796, 508, 642, 444, 476, 208, 500,\n",
       "        462, 340, 816, 378, 494, 686, 698, 424, 620, 252, 574, 468, 600, 814,\n",
       "        698, 448, 516, 562, 298, 680, 564, 624, 704, 484, 512, 342, 402, 786,\n",
       "        596, 490, 714, 860, 802, 402, 624, 408])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * ctc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([192, 827, 560, 843, 753, 622, 782, 692, 725, 361, 605, 644, 643, 160,\n",
       "        393, 677, 696, 375, 828, 257, 125, 796, 507, 641, 444, 476, 207, 499,\n",
       "        461, 339, 815, 378, 493, 686, 697, 423, 620, 252, 573, 468, 600, 814,\n",
       "        698, 448, 515, 562, 298, 680, 563, 624, 704, 483, 511, 341, 401, 785,\n",
       "        596, 490, 714, 859, 802, 402, 623, 408])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  1,  1,  1,  2,  2,  5,  2,  1,  4,  4,  1,  3,  2,  1,\n",
       "        3,  1,  1,  1,  3,  1,  3,  3,  2,  4,  1,  2,  5,  3,  2,  2,  3,\n",
       "        3,  2, 20, 97])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PadProcesser:\n",
    "    def __init__(self, labels):\n",
    "        labels = labels + ['~']\n",
    "        self.blank_id = len(labels) - 1\n",
    "        self.space_id = labels.index(' ')\n",
    "        self.labels_map = dict([(i, labels[i]) for i in range(len(labels))])\n",
    "    \n",
    "    def bound_text(self, tokens):\n",
    "        return [self.space_id] + tokens + [self.space_id]\n",
    "    \n",
    "    def bound_ctc(self, tokens, logprobs):\n",
    "        tokens = [self.space_id, self.blank_id] + tokens + [self.blank_id, self.space_id]\n",
    "        \n",
    "        logprobs = np.lib.pad(logprobs, ((2, 2), (0, 0)), 'edge')\n",
    "\n",
    "        def swap(col, a, b):\n",
    "            logprobs[col][a], logprobs[col][b] = logprobs[col][b], logprobs[col][a]\n",
    "        \n",
    "        first_token, last_token = tokens[2], tokens[-3]\n",
    "        swap(0, first_token, self.space_id)\n",
    "        swap(1, first_token, self.blank_id)\n",
    "        swap(-1, last_token, self.space_id)\n",
    "        swap(-2, last_token, self.blank_id)\n",
    "\n",
    "        return tokens, logprobs\n",
    "    \n",
    "    def merge(self, tokens):\n",
    "        output_tokens = []\n",
    "        output_cnts = []\n",
    "        cnt = 0\n",
    "        for i in range(len(tokens)):\n",
    "            if i != 0 and (tokens[i - 1] != tokens[i]):\n",
    "                output_tokens.append(tokens[i - 1])\n",
    "                output_cnts.append(cnt)\n",
    "\n",
    "                cnt = 0\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "        output_tokens.append(tokens[-1])\n",
    "        output_cnts.append(cnt)\n",
    "        \n",
    "        assert sum(output_cnts) == len(tokens), f'SUM_CHECK {sum(output_cnts)} vs {len(tokens)}'\n",
    "\n",
    "        return output_tokens, output_cnts\n",
    "    \n",
    "    def merge_with_blanks(self, tokens, cnts, logprobs=None):\n",
    "        def choose_sep(l, r, a, b):\n",
    "            # `tokens[l] == a and tokens[r] == b`.\n",
    "            sum_a, sum_b = logprobs[l, a], logprobs[l + 1:r + 1, b].sum()\n",
    "            best_sum, best_sep = sum_a + sum_b, 0\n",
    "            for sep in range(1, r - l):\n",
    "                sum_a += logprobs[l + sep, a]\n",
    "                sum_b -= logprobs[l + sep, b]\n",
    "                if sum_a + sum_b > best_sum:\n",
    "                    best_sum, best_sep = sum_a + sum_b, sep\n",
    "\n",
    "            return best_sep\n",
    "        \n",
    "        output_tokens = []\n",
    "        output_durs = []\n",
    "        blank_cnt = 0\n",
    "        total_cnt = 0\n",
    "        for token, cnt in zip(tokens, cnts):\n",
    "            total_cnt += cnt\n",
    "            if token == self.blank_id:\n",
    "                blank_cnt += cnt\n",
    "                continue\n",
    "            \n",
    "            output_tokens.append(token)\n",
    "            \n",
    "            if logprobs is None:\n",
    "                # Half half.\n",
    "                left_cnt = blank_cnt // 2\n",
    "            else:\n",
    "                # Clever sep choice based on sum of log probs.\n",
    "                left_cnt = choose_sep(\n",
    "                    l=total_cnt - cnt - blank_cnt - 1,\n",
    "                    r=total_cnt - cnt,\n",
    "                    a=output_tokens[-1],\n",
    "                    b=token,\n",
    "                )\n",
    "            right_cnt = blank_cnt - left_cnt\n",
    "            blank_cnt = 0\n",
    "            \n",
    "            if left_cnt:\n",
    "                output_durs[-1] += left_cnt\n",
    "            output_durs.append(cnt + right_cnt)\n",
    "        \n",
    "        output_durs[-1] += blank_cnt\n",
    "\n",
    "        assert sum(output_durs) == sum(cnts), f'SUM_CHECK {sum(output_durs)} vs {sum(cnts)}'\n",
    "\n",
    "        return output_tokens, output_durs\n",
    "    \n",
    "    def align(self, output_tokens, gt_text):\n",
    "        def make_str(tokens):\n",
    "            return ''.join(self.labels_map[c] for c in tokens)\n",
    "        \n",
    "        s = make_str(output_tokens)\n",
    "        t = make_str(gt_text)\n",
    "        alignmet = pairwise2.align.globalxx(s, t, gap_char='%')[0]\n",
    "        sa, ta, *_ = alignmet\n",
    "        return sa, ta\n",
    "    \n",
    "    def generate(self, gt_text, alignment, durs):\n",
    "        output_tokens = []\n",
    "        output_cnts = []\n",
    "        si, ti = 0, 0\n",
    "#         print(len(durs))\n",
    "        assert len(alignment[0]) == len(alignment[1])\n",
    "        for sc, tc in zip(*alignment):\n",
    "#             print(si, sc, ti, tc)\n",
    "            if sc == '%' and tc == '%':\n",
    "                print('NO WAY')\n",
    "                continue\n",
    "            \n",
    "            if sc == '%':\n",
    "                output_tokens.append(self.blank_id)\n",
    "                output_cnts.append(durs[si])\n",
    "                si += 1\n",
    "            elif tc == '%':\n",
    "                output_tokens.append(gt_text[ti])\n",
    "                output_cnts.append(0)\n",
    "                ti += 1\n",
    "            else:\n",
    "                output_tokens.append(gt_text[ti])\n",
    "                output_cnts.append(durs[si])\n",
    "                si += 1\n",
    "                ti += 1\n",
    "\n",
    "        assert sum(output_cnts) == sum(durs)\n",
    "        \n",
    "        return output_tokens, output_cnts\n",
    "\n",
    "    def __call__(self, text, ctc_tokens, ctc_logprobs, mel_len):\n",
    "        # This adds +2 tokens.\n",
    "        text = self.bound_text(text)\n",
    "        # This add +4 tokens, 2 of them are blank.\n",
    "        ctc_tokens, ctc_logprobs = self.bound_ctc(ctc_tokens, ctc_logprobs)\n",
    "\n",
    "        ctc_tokens, ctc_cnts = self.merge(ctc_tokens)\n",
    "        ctc_tokens, ctc_durs = self.merge_with_blanks(ctc_tokens, ctc_cnts, ctc_logprobs)\n",
    "        \n",
    "        alignment = self.align(text, ctc_tokens)\n",
    "        tokens, cnts = self.generate(text, alignment, ctc_durs)\n",
    "        tokens, durs = self.merge_with_blanks(tokens, cnts)\n",
    "        assert tokens == text, 'EXACT_TOKENS_MATCH_CHECK'\n",
    "\n",
    "        def adjust(start, direction, value):\n",
    "            i = start\n",
    "            while value != 0:\n",
    "                dur = durs[i]\n",
    "                \n",
    "                if value < 0:\n",
    "                    durs[i] = dur - value\n",
    "                else:\n",
    "                    durs[i] = max(dur - value, 0)\n",
    "                \n",
    "                value -= dur - durs[i]\n",
    "                i += direction\n",
    "\n",
    "        adjust(0, 1, 4)\n",
    "        adjust(-1, -1, sum(durs) - mel_len)  # Including 4 suffix bound tokens.\n",
    "        assert durs[0] >= 0, f'{durs[0]}'\n",
    "        assert durs[-1] >= 0, f'{durs[-1]}'\n",
    "        \n",
    "        durs = np.array(durs, dtype=np.long)\n",
    "        assert durs.shape[0] == len(text), f'LEN_CHECK {durs.shape[0]} vs {len(text)}'\n",
    "        assert np.sum(durs) == mel_len, f'SUM_CHECK {np.sum(durs)} vs {mel_len}'\n",
    "\n",
    "        return durs\n",
    "\n",
    "processer = PadProcesser(labels)\n",
    "durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "durs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq:\n",
    "    def __init__(self, tokens, cnts=None):\n",
    "        if cnts is None:\n",
    "            cnts = np.ones(len(tokens), dtype=np.long)\n",
    "\n",
    "        assert len(tokens) == len(cnts)\n",
    "        self.tokens = tokens\n",
    "        self.cnts = cnts\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(list(zip(self.tokens, self.cnts)))\n",
    "    \n",
    "    @property\n",
    "    def total(self):\n",
    "        return sum(self.cnts)\n",
    "    \n",
    "    def merge(self):\n",
    "        output_tokens = []\n",
    "        output_cnts = []\n",
    "        \n",
    "        cnt = 0\n",
    "        for i in range(len(self.tokens)):\n",
    "            if i != 0 and (self.tokens[i - 1] != self.tokens[i]):\n",
    "                output_tokens.append(self.tokens[i - 1])\n",
    "                output_cnts.append(cnt)\n",
    "\n",
    "                cnt = 0\n",
    "\n",
    "            cnt += self.cnts[i]\n",
    "\n",
    "        output_tokens.append(self.tokens[-1])\n",
    "        output_cnts.append(cnt)\n",
    "        \n",
    "        assert sum(output_cnts) == sum(self.cnts), \\\n",
    "            f'SUM-CHECK {sum(output_cnts)} vs {sum(self.cnts)}'\n",
    "\n",
    "        return Seq(output_tokens, output_cnts)\n",
    "    \n",
    "    def full_pad(self, blank_id, blank_cnt=1):\n",
    "        output_tokens = [blank_id]\n",
    "        output_cnts = [blank_cnt]\n",
    "\n",
    "        for token, cnt in zip(self.tokens, self.cnts):\n",
    "            output_tokens.append(token)\n",
    "            output_cnts.append(cnt)\n",
    "            \n",
    "            output_tokens.append(blank_id)\n",
    "            output_cnts.append(blank_cnt)\n",
    "        \n",
    "        return Seq(output_tokens, output_cnts)\n",
    "    \n",
    "    def adjust_cnt(self, value, start=-1, direction='left'):\n",
    "        tokens, cnts = self.tokens, self.cnts.copy()\n",
    "        \n",
    "        i, di = start, -1 if direction == 'left' else 1\n",
    "        while value != 0:\n",
    "            cnt = cnts[i]\n",
    "\n",
    "            if value < 0:\n",
    "                cnts[i] = cnt - value\n",
    "            else:\n",
    "                cnts[i] = max(cnt - value, 0)\n",
    "\n",
    "            value -= cnt - cnts[i]\n",
    "            i += di\n",
    "        \n",
    "        return Seq(tokens, cnts)\n",
    "    \n",
    "    def split2(self):\n",
    "        tokens1, cnts1 = [], []\n",
    "        tokens2, cnts2 = [], []\n",
    "        turn = 1\n",
    "        \n",
    "        for token, cnt in zip(self.tokens, self.cnts):\n",
    "            if turn == 1:\n",
    "                tokens1.append(token)\n",
    "                cnts1.append(cnt)\n",
    "            else:\n",
    "                tokens2.append(token)\n",
    "                cnts2.append(cnt)\n",
    "            \n",
    "            turn = 1 if turn == 2 else 2\n",
    "        \n",
    "        return Seq(tokens1, cnts1), Seq(tokens2, cnts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  1,  3,  2,  0,  2,  2,  0,  0,  1,  0,  0,\n",
       "         0,  0,  0,  0,  0,  2,  2,  0,  3,  0,  0,  2,  2,  0,  0,  2,  2,\n",
       "         0,  3, 15]),\n",
       " array([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1,\n",
       "        1, 1, 2, 1, 1, 2, 3, 1, 2, 1, 2, 1, 1, 2]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FullProcessor(PadProcesser):\n",
    "    def __call__(self, text, ctc_tokens, ctc_logprobs, mel_len):\n",
    "        text = Seq(text).full_pad(self.blank_id)\n",
    "        ctc = Seq(ctc_tokens).merge().full_pad(self.blank_id, blank_cnt=0).merge()\n",
    "        \n",
    "        alignment = self.align(text.tokens, ctc.tokens)\n",
    "        gen = Seq(*self.generate(text.tokens, alignment, ctc.cnts)).merge()\n",
    "#         gen = gen.merge().adjust_cnt(gen.total - mel_len)\n",
    "        \n",
    "        # Two durs conditions.\n",
    "        assert gen.tokens == text.tokens\n",
    "#         assert gen.total == mel_len\n",
    "        assert abs(2 * gen.total - mel_len) <= 1\n",
    "        \n",
    "        blanks, text = gen.split2()\n",
    "        blanks = np.array(blanks.cnts, dtype=np.long)\n",
    "        cnts = np.array(text.cnts, dtype=np.long)\n",
    "        \n",
    "        assert len(blanks) == len(cnts) + 1\n",
    "        \n",
    "        return blanks, cnts\n",
    "\n",
    "\n",
    "processer = FullProcessor(labels)\n",
    "durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "durs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt(text):\n",
    "    print(*(ctc_labels[c] for c in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: t h e _ p r o m i n e n t _ e v i l s _ o f _ t h i s _ p r i s o n _ n e w g a t e _ e v i l s _ w h i c h _ t h e _ a l t e r a t i o n s _ m a d e _ w i t h i n _ t h e _ l a s t _ f o u r _ y e a r s _ h a v e _ f a i l e d _ t o _ r e m o v e\n",
      "CTC: t h ~ p r o ~ m ~ i ~ n ~ e n ~ t ~ _ ~ e ~ v ~ i l ~ s ~ _ ~ o f _ t h i s ~ _ ~ p r ~ i ~ s ~ o n ~ _ ~ n ~ e w ~ g ~ a ~ t e ~ _ ~ e ~ v ~ i l ~ s ~ _ w h i c h ~ _ t h e _ ~ a l ~ t ~ e r ~ a ~ t i ~ o n ~ s ~ _ ~ m ~ a ~ d e _ w i t h ~ i n ~ _ t h e _ ~ l ~ a s t ~ _ ~ f ~ o u r ~ _ ~ y ~ e a r ~ s ~ _ h a v e ~ _ ~ f ~ a i ~ l ~ e d ~ _ t o ~ _ ~ r ~ e ~ m ~ o ~ v e ~\n"
     ]
    }
   ],
   "source": [
    "ctc_labels = ['_'] + labels[1:] + ['~']\n",
    "print('GT:', *(ctc_labels[c] for c in text1))\n",
    "print('CTC:', *(ctc_labels[c] for c, _ in itertools.groupby(ctc_tokens1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: \" t h e _ p r o m i n e n t _ e v i l s _ o f _ t h i s _ p r i s o n _ ( n e w g a t e ) _ - - _ e v i l s _ w h i c h _ t h e _ a l t e r a t i o n s _ m a d e _ w i t h i n _ t h e _ l a s t _ f o u r _ y e a r s _ h a v e _ f a i l e d _ t o _ r e m o v e\n"
     ]
    }
   ],
   "source": [
    "ctc_labels = ['_'] + list(gconfig.labels[1:]) + ['~']\n",
    "print('GT:', *(ctc_labels[c] for c in FT[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('which existed in the various prisons%',\n",
       " 'which existed in the various prisons,')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''.join(labels[c] for c in text1)\n",
    "t = ''.join(gconfig.labels[c] for c in FT[k])\n",
    "alignment = pairwise2.align.globalxx(s, t, gap_char='%')[0][:2]\n",
    "alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  1,  3,  2,  0,  2,  2,  0,  0,  1,  0,  0,\n",
       "         0,  0,  0,  0,  0,  2,  2,  0,  3,  0,  0,  2,  2,  0,  0,  2,  2,\n",
       "         0,  3,  0, 14]),\n",
       " array([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1,\n",
       "        1, 1, 2, 1, 1, 2, 3, 1, 2, 1, 2, 1, 1, 2, 1]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_st(s, t, b, d):\n",
    "    b, d, total = b.copy(), d.copy(), b.sum() + d.sum()\n",
    "    si, ti = 0, 0\n",
    "    new_b, new_d, k, tokens = [], [], 0, []\n",
    "    \n",
    "    s = ''.join(labels[c] for c in s)\n",
    "    t = ''.join(gconfig.labels[c] for c in t)\n",
    "    alignment = pairwise2.align.globalxx(s, t, gap_char='%')[0][:2]\n",
    "    \n",
    "    for sc, tc in zip(*alignment):\n",
    "        if sc == '%' and tc == '%':\n",
    "            print('NO WAY')\n",
    "            continue\n",
    "\n",
    "        if sc == '%':\n",
    "            new_b.append(0)\n",
    "            new_d.append(0)\n",
    "            ti += 1\n",
    "        elif tc == '%':\n",
    "            b[si + 1] += b[si] + d[si]\n",
    "            si += 1\n",
    "        else:\n",
    "            new_b.append(b[si])\n",
    "            new_d.append(d[si])\n",
    "            si += 1\n",
    "            ti += 1\n",
    "\n",
    "    new_b.append(b[-1])\n",
    "    \n",
    "    b, d = np.array(new_b), np.array(new_d)\n",
    "\n",
    "    assert b.sum() + d.sum() == total\n",
    "    \n",
    "    (b, d), _ = commons.adjust_durs(b, d)\n",
    "\n",
    "    assert b.sum() + d.sum() == total\n",
    "\n",
    "    return b, d\n",
    "\n",
    "        \n",
    "align_st(text1, FT[k], *durs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def expand_punct(b, d, t, ft):\n",
    "#     total = b.sum() + d.sum()\n",
    "#     b, d = b.copy(), d.copy()\n",
    "    \n",
    "#     assert b.sum() + d.sum() == total\n",
    "    \n",
    "#     new_b, new_d, k = [], [], 0\n",
    "#     for i in range(len(ft)):\n",
    "#         c = ft[i]\n",
    "#         l = gconfig.labels[c]\n",
    "#         if l in labels and l == t[k]:\n",
    "#             new_b.append(b[k])\n",
    "#             new_d.append(d[k])\n",
    "#             k += 1\n",
    "#         elif l in (':', '-', '.', ')') and k < len(t) and labels[t[k]] == ' ' and ft[i + 1] != t[k]:\n",
    "#             new_b.append(b[k])\n",
    "#             new_d.append(d[k])\n",
    "#             k += 1\n",
    "#         else:\n",
    "#             new_b.append(b[k] // 2)\n",
    "#             b[k] -= (b[k] // 2)\n",
    "#             new_d.append(0)\n",
    "    \n",
    "#     new_b.append(b[-1])\n",
    "    \n",
    "#     b, d = np.array(new_b), np.array(new_d)\n",
    "\n",
    "#     assert b.sum() + d.sum() == total\n",
    "    \n",
    "#     (b, d), _ = commons.adjust_durs(b, d)\n",
    "\n",
    "#     assert b.sum() + d.sum() == total\n",
    "\n",
    "#     return b, d\n",
    "\n",
    "\n",
    "# expand_punct(*durs1, text1, FT[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = lambda: processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "# n, m = 10000, 280000\n",
    "# (timeit.timeit(f, number=n) / n) * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_batch(batch):\n",
    "#     text = batch[2].numpy()\n",
    "#     text_len = batch[3].numpy()\n",
    "#     ctc_tokens = batch[1].numpy()\n",
    "#     ctc_logprobs = batch[0].numpy()\n",
    "#     ctc_len = batch[4].numpy()\n",
    "#     mel_len = batch[-1].numpy()\n",
    "\n",
    "#     durs = []\n",
    "#     for text1, text_len1, ctc_tokens1, ctc_logprobs1, ctc_len1, mel_len1 in zip(\n",
    "#         text, text_len, ctc_tokens, ctc_logprobs, ctc_len, mel_len\n",
    "#     ):\n",
    "#         text1 = list(text1[:text_len1])\n",
    "#         ctc_tokens1 = list(ctc_tokens1[:ctc_len1])\n",
    "#         ctc_logprobs1 = ctc_logprobs1[:ctc_len1]\n",
    "#         mel_len1 = mel_len1.item()\n",
    "        \n",
    "#         assert mel_len1 == ctc_len1\n",
    "\n",
    "#         durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "        \n",
    "#         durs1, doable = commons.adjust_durs(*durs1)\n",
    "#         if doable:\n",
    "#             assert (durs1[-1] == 0).sum() == 0\n",
    "        \n",
    "#         assert sum(durs1[0]) + sum(durs1[1]) == mel_len1\n",
    "        \n",
    "#         durs.append(durs1)\n",
    "    \n",
    "#     return durs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296aed4b633d4cd68297139834d0396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it = tqdm.notebook.tqdm(zip(*evaluated_tensors), total=len(data_layer) // 64)\n",
    "# batches = Parallel(n_jobs=16)(delayed(process_batch)(batch) for batch in it)\n",
    "# len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durs = []\n",
    "for batch in batches:\n",
    "    durs.extend(batch)\n",
    "\n",
    "len(durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs_dir = pathlib.Path('/home/stanislavv/data/librimeta/durs/ljspeech_300epochs-qn15x5-eqlen_all-1s_split3')\n",
    "name = f'test'\n",
    "durs_dir.mkdir(exist_ok=True)\n",
    "np.save(durs_dir / f'{name}.npy', durs, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f525eaea414b7d86bd188acf4c6184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This is going pretty fast.\n",
    "durs_dir = pathlib.Path('/home/stanislavv/data/librimeta/durs/ljspeech_original-qn-15x5_2x-less')\n",
    "name = f'test'\n",
    "durs_dir.mkdir(exist_ok=True)\n",
    "k = -1\n",
    "\n",
    "durs = []\n",
    "for batch in tqdm.notebook.tqdm(zip(*evaluated_tensors), total=len(data_layer) // 64):\n",
    "    text = batch[2].numpy()\n",
    "    text_len = batch[3].numpy()\n",
    "    ctc_tokens = batch[1].numpy()\n",
    "    ctc_logprobs = batch[0].numpy()\n",
    "    ctc_len = batch[4].numpy()\n",
    "    mel_len = batch[-1].numpy()\n",
    "\n",
    "    for text1, text_len1, ctc_tokens1, ctc_logprobs1, ctc_len1, mel_len1 in zip(\n",
    "        text, text_len, ctc_tokens, ctc_logprobs, ctc_len, mel_len\n",
    "    ):\n",
    "        text1 = list(text1[:text_len1])\n",
    "        ctc_tokens1 = list(ctc_tokens1[:ctc_len1])\n",
    "        ctc_logprobs1 = ctc_logprobs1[:ctc_len1]\n",
    "        mel_len1 = mel_len1.item()\n",
    "        \n",
    "#         assert mel_len1 == ctc_len1\n",
    "\n",
    "        durs1 = processer(text1, ctc_tokens1, ctc_logprobs1, mel_len1)\n",
    "        \n",
    "#         durs1 = commons.adjust_durs(*durs1)\n",
    "        k += 1\n",
    "        durs1 = align_st(text1, FT[k], *durs1)\n",
    "        assert (durs1[-1] == 0).sum() == 0\n",
    "        \n",
    "        assert 2 * (sum(durs1[0]) + sum(durs1[1])) - mel_len1 <= 1\n",
    "        \n",
    "        durs.append(durs1)\n",
    "\n",
    "\n",
    "np.save(durs_dir / f'{name}.npy', durs, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s o _ j o e _ w a s _ w h i p p e d _ b e c a u s e _ h e _ a l l o w e d _ t h e _ t u r k e y _ t o _ g e t _ i n t o _ t h e _ p o t .\n"
     ]
    }
   ],
   "source": [
    "tt(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(b) + sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6,  0,  0,  0,  0,  0,  5,  0,  0,  0,  3,  2,  3,  0,  5,  1,  2,\n",
       "         5,  0,  1, 18,  2,  1,  1,  8, 41,  4,  6,  1,  2,  1, 12,  2,  4,\n",
       "         6,  1,  0,  1,  3,  1,  2,  9,  2,  0,  2,  5,  2,  9,  4,  0,  3,\n",
       "         9,  2,  2, 17,  4,  0,  0,  2,  4,  0,  3,  2,  8,  0,  0,  2,  0,\n",
       "         0,  9,  1,  0,  0,  6,  3,  1,  3,  3,  2,  2,  4,  1,  2,  7,  0,\n",
       "         2, 34,  5,  2,  4,  0,  7,  3,  0,  2,  3,  0, 12,  3,  0,  8,  5,\n",
       "         8,  0, 13,  2,  4,  2,  3, 70,  5,  2,  6,  0,  8,  2,  6,  2,  0,\n",
       "         5,  0,  0,  0,  8,  4,  7,  2,  5,  1,  2,  6,  0,  0,  0,  4,  5,\n",
       "         4,  3,  7,  2,  0,  0, 36,  5,  1,  0,  3,  2,  0,  0,  6,  2,  3,\n",
       "         9,  3,  0,  2,  2, 12,  4,  0,  1,  5,  2, 14,  4,  2, 11,  4,  2,\n",
       "         5,  7,  1,  4,  2,  0,  4,  3,  1,  0, 11,  2,  2,  4,  4,  1,  2,\n",
       "        12,  4,  2,  3,  1,  2,  8,  5,  8,  0, 13,  3,  2,  8,  6,  2,  5,\n",
       "         1,  1,  2,  2,  0,  1,  2,  0,  5,  2,  2,  8,  0,  0,  0,  7,  1,\n",
       "         2,  4,  0,  2,  9,  4,  1,  8,  0, 34]),\n",
       " array([2, 3, 2, 3, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "        1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "        2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
       "        1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1,\n",
       "        2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1,\n",
       "        2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
       "        1, 1, 2, 1, 1, 2, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 69)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, d = durs1\n",
    "sum(b), len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6,  0,  0,  0,  0,  0,  5,  0,  0,  0,  3,  2,  3,  0,  5,  1,  2,\n",
       "         5,  0,  1, 18,  2,  1,  1,  8, 41,  4,  6,  1,  2,  1, 12,  2,  4,\n",
       "         6,  1,  0,  1,  3,  1,  2,  9,  2,  0,  2,  5,  2,  9,  4,  0,  3,\n",
       "         9,  2,  2, 17,  4,  0,  0,  2,  4,  0,  3,  2,  8,  0,  0,  2,  0,\n",
       "         0,  9,  1,  0,  0,  6,  3,  1,  3,  3,  2,  2,  4,  1,  2,  7,  0,\n",
       "         2, 34,  5,  2,  4,  0,  7,  3,  0,  2,  3,  0, 12,  3,  0,  8,  5,\n",
       "         8,  0, 13,  2,  4,  2,  3, 70,  5,  2,  6,  0,  8,  2,  6,  2,  0,\n",
       "         5,  0,  0,  0,  8,  4,  7,  2,  5,  1,  2,  6,  0,  0,  0,  4,  5,\n",
       "         4,  3,  7,  2,  0,  0, 36,  5,  1,  0,  3,  2,  0,  0,  6,  2,  3,\n",
       "         9,  3,  0,  2,  2, 12,  4,  0,  1,  5,  2, 14,  4,  2, 11,  4,  2,\n",
       "         5,  7,  1,  4,  2,  0,  4,  3,  1,  0, 11,  2,  2,  4,  4,  1,  2,\n",
       "        12,  4,  2,  3,  1,  2,  8,  5,  8,  0, 13,  3,  2,  8,  6,  2,  5,\n",
       "         1,  1,  2,  2,  0,  1,  2,  0,  5,  2,  2,  8,  0,  0,  0,  7,  1,\n",
       "         2,  4,  0,  2,  9,  4,  1,  8,  0, 34]),\n",
       " array([2, 3, 2, 3, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "        1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "        2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
       "        1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1,\n",
       "        2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1,\n",
       "        2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
       "        1, 1, 2, 1, 1, 2, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjust_durs(*durs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,   15,   16,   17,   18,   19,   20,   21,   22,   23,   24,\n",
       "         25,   26,   27,   28,   29,   30,   31,   32,   33,   34,   35,\n",
       "         36,   37,   38,   39,   40,   41,   42,   43,   44,   45,   46,\n",
       "         47,   48,   49,   50,   51,   52,   53,   54,   55,   56,   57,\n",
       "         58,   59,   60,   61,   62,   63,   64,   65,   66,   67,   68,\n",
       "         69,   70,   71,   72,   73,   74,   75,   76,   77,   78,   79,\n",
       "         80,   81,   82,   83,   84,   85,   86,   87,   88,   89,   90,\n",
       "         91,   92,   93,   94,   95,   96,   97,   98,   99,  100,  101,\n",
       "        102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  112,\n",
       "        113,  114,  116,  117,  118,  120,  122,  124,  126,  128,  130,\n",
       "        131,  132,  134,  136,  138,  140,  142,  144,  146,  148,  150,\n",
       "        152,  154,  156,  158,  160,  162,  164,  166,  168,  170,  172,\n",
       "        174,  176,  178,  180,  182,  184,  186,  188,  190,  192,  194,\n",
       "        196,  198,  200,  202,  204,  206,  208,  210,  212,  214,  216,\n",
       "        218,  220,  222,  224,  226,  228,  230,  232,  234,  236,  238,\n",
       "        240,  242,  244,  246,  248,  250,  252,  254,  256,  258,  260,\n",
       "        262,  264,  266,  268,  270,  272,  274,  276,  278,  280,  282,\n",
       "        284,  286,  288,  290,  292,  294,  296,  298,  300,  302,  304,\n",
       "        306,  308,  310,  312,  314,  316,  318,  320,  322,  324,  326,\n",
       "        328,  330,  332,  334,  336,  338,  340,  342,  344,  346,  348,\n",
       "        350,  352,  354,  356,  358,  360,  362,  364,  366,  368,  370,\n",
       "        372,  374,  376,  378,  380,  382,  384,  386,  388,  390,  392,\n",
       "        394,  396,  398,  400,  402,  404,  406,  408,  410,  412,  414,\n",
       "        416,  418,  420,  422,  424,  426,  428,  430,  432,  436,  438,\n",
       "        440,  442,  446,  448,  452,  454,  456,  458,  460,  462,  464,\n",
       "        466,  470,  472,  474,  476,  482,  484,  488,  490,  492,  494,\n",
       "        502,  508,  510,  512,  514,  518,  522,  524,  526,  534,  538,\n",
       "        540,  554,  558,  568,  586,  590,  592,  602,  630,  644,  652,\n",
       "        666,  670,  676,  684,  760,  816,  840, 1130])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(max_blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(b.max() for b, _ in durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(d.max() for _, d in durs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1167549, 49408497, 2.363053059476794)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num, total = 0, 0\n",
    "for b, _ in durs:\n",
    "    num += (b > 30).sum()\n",
    "    total += len(b)\n",
    "\n",
    "num, total, (num / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31619, 49129870, 0.06435799646935765)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num, total = 0, 0\n",
    "for _, d in durs:\n",
    "    num += (d > 15).sum()\n",
    "    total += len(d)\n",
    "\n",
    "num, total, (num / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/278627 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 14127/278627 [00:00<00:01, 141258.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 26700/278627 [00:00<00:01, 136209.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 36747/278627 [00:00<00:01, 123074.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 49495/278627 [00:00<00:01, 124361.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 62138/278627 [00:00<00:01, 124971.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 74744/278627 [00:00<00:01, 125295.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 87506/278627 [00:00<00:01, 125982.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 100505/278627 [00:00<00:01, 127157.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 113237/278627 [00:00<00:01, 127205.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 125793/278627 [00:01<00:01, 126704.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 138547/278627 [00:01<00:01, 126951.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 151085/278627 [00:01<00:01, 126474.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 163550/278627 [00:01<00:00, 125234.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 175949/278627 [00:01<00:00, 123761.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 188242/278627 [00:01<00:00, 122351.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 200423/278627 [00:01<00:00, 121550.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 212541/278627 [00:01<00:00, 120701.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 224767/278627 [00:01<00:00, 121163.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 237060/278627 [00:01<00:00, 121684.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 249378/278627 [00:02<00:00, 122127.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 261584/278627 [00:02<00:00, 121933.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 278627/278627 [00:02<00:00, 123477.48it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(\n",
    "    list(itertools.chain.from_iterable(b for b, _ in tqdm.tqdm(durs))),\n",
    "    bins=range(20),\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
