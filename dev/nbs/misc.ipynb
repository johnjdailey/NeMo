{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a[10:] - a[:-10]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 3, 1, 3, 4, 4, 3, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(5, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "       ...  \n",
       "995    931.5\n",
       "996    932.5\n",
       "997    933.5\n",
       "998    934.5\n",
       "999    935.5\n",
       "Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.arange(1000)).rolling(128).mean().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.zeros(10), np.ones(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64,  86,  84,  80,  66,  97,   7,   3,  89,  93,  11,  88,   6,\n",
       "       106,  28,  29, 110, 117,   0,  19,  49,   1,   8,  48,  30, 120,\n",
       "        77,  20,  98,  24,  47,  63, 141,  18,  59,  12,  14, 133,  60,\n",
       "        61, 111,  45,  99, 107,  35,  10,  81,   4,  21,  26,  85,  39,\n",
       "         9,  15, 140,  54,  13,  36,   5,  58,  51,  43,  17, 148, 155,\n",
       "       103,   2,  46,  92, 101, 121, 187,  37,  34,  67,  27,  41,  23,\n",
       "       137, 184,  32,  95,  56,  68,  44,  94,  22, 144, 182, 196, 162,\n",
       "        70,  50,  16,  75,  62, 194,  91,  33, 151,  25,  31, 145,  42,\n",
       "       139,  38, 175, 215, 100,  57,  79,  40,  73,  53, 123, 161, 112,\n",
       "        52, 183,  69,  74,  96, 129,  55, 124, 170, 169, 153, 234, 172,\n",
       "       229,  78, 108, 204, 193, 206, 232, 191, 219,  72, 102, 201, 218,\n",
       "       146, 163, 127, 198, 270,  65, 119, 233, 258, 230,  83, 156, 265,\n",
       "       209, 138, 135, 214, 113, 122, 171, 131,  87, 276, 225, 228, 203,\n",
       "       247, 249, 211,  71, 250, 237, 109, 178,  82, 244, 157, 261,  76,\n",
       "       263, 176, 164, 284, 116, 290, 143, 152, 136, 158, 202, 248, 269,\n",
       "       199, 231, 205, 293, 256, 125, 128,  90, 154, 254, 220, 115, 297,\n",
       "       251, 104, 159, 126, 242, 259, 280, 114, 227, 264, 296, 173, 278,\n",
       "       260, 105, 324, 239, 271, 166, 177, 147, 134, 118, 267, 208, 167,\n",
       "       295, 282, 342, 317, 236, 274, 336, 255, 338, 142, 238, 132, 253,\n",
       "       180, 291, 197, 221, 192, 130, 189, 345, 288, 226, 212, 347, 195,\n",
       "       351, 160, 188, 382, 150, 279, 331, 309, 323, 235, 149, 285, 384,\n",
       "       300, 352, 213, 286, 301, 185, 207, 370, 246, 240, 361, 277, 243,\n",
       "       165, 294, 181, 289, 321, 372, 168, 335, 179, 411, 174, 322, 373,\n",
       "       430, 348, 408, 356, 190, 200, 310, 412, 369, 333, 306, 403, 387,\n",
       "       359, 442, 383, 268, 186, 440, 425, 314, 349, 308, 389, 216, 340,\n",
       "       386, 325, 257, 312, 443, 262, 451, 303, 422, 210, 407, 393, 266,\n",
       "       217, 445, 426, 287, 355, 434, 252, 378, 307, 299, 471, 420, 223,\n",
       "       358, 272, 313, 467, 292, 470, 399, 463, 222, 397, 241, 343, 224,\n",
       "       417, 488, 418, 302, 448, 429, 245, 273, 311, 409, 275, 395, 328,\n",
       "       330, 346, 458, 377, 298, 492, 350, 281, 316, 474, 363, 344, 428,\n",
       "       496, 455, 364, 415, 318, 337, 327, 283, 508, 502, 396, 490, 523,\n",
       "       390, 357, 320, 304, 354, 365, 421, 305, 388, 381, 419, 332, 404,\n",
       "       513, 341, 444, 510, 326, 457, 509, 465, 406, 329, 439, 353, 366,\n",
       "       519, 514, 552, 319, 315, 401, 479, 446, 452, 380, 339, 362, 334,\n",
       "       529, 515, 374, 476, 416, 499, 505, 402, 432, 413, 557, 360, 376,\n",
       "       498, 400, 367, 459, 565, 536, 424, 550, 371, 478, 368, 410, 547,\n",
       "       571, 562, 554, 441, 477, 431, 486, 464, 392, 385, 579, 551, 375,\n",
       "       544, 460, 569, 535, 583, 485, 586, 558, 503, 394, 525, 379, 518,\n",
       "       517, 559, 391, 511, 462, 435, 453, 614, 468, 438, 487, 481, 414,\n",
       "       588, 454, 582, 495, 605, 524, 483, 589, 521, 427, 466, 613, 600,\n",
       "       398, 449, 437, 436, 433, 405, 561, 456, 598, 423, 528, 447, 516,\n",
       "       593, 507, 469, 541, 461, 493, 596, 534, 504, 500, 671, 662, 568,\n",
       "       643, 654, 556, 651, 640, 489, 472, 473, 564, 494, 577, 664, 618,\n",
       "       539, 545, 530, 475, 646, 702, 527, 680, 480, 450, 629, 677, 497,\n",
       "       601, 684, 663, 597, 670, 546, 639, 659, 673, 694, 484, 482, 623,\n",
       "       703, 718, 644, 543, 532, 627, 705, 634, 657, 520, 587, 674, 695,\n",
       "       537, 491, 567, 691, 526, 599, 522, 540, 619, 538, 681, 725, 611,\n",
       "       658, 570, 699, 512, 723, 604, 560, 506, 591, 692, 580, 574, 501,\n",
       "       711, 531, 607, 736, 595, 667, 710, 549, 636, 751, 661, 713, 712,\n",
       "       573, 747, 542, 603, 635, 555, 760, 581, 714, 575, 698, 638, 621,\n",
       "       701, 739, 533, 592, 726, 678, 648, 630, 578, 633, 715, 656, 594,\n",
       "       740, 724, 616, 685, 655, 553, 693, 785, 722, 632, 548, 576, 642,\n",
       "       666, 668, 775, 770, 771, 752, 777, 606, 563, 781, 794, 754, 612,\n",
       "       758, 645, 744, 637, 585, 707, 748, 572, 650, 660, 584, 782, 615,\n",
       "       602, 566, 665, 649, 590, 719, 620, 750, 669, 688, 804, 687, 817,\n",
       "       829, 806, 801, 828, 749, 686, 729, 709, 608, 778, 682, 742, 696,\n",
       "       624, 622, 609, 610, 813, 625, 717, 765, 675, 626, 700, 766, 853,\n",
       "       617, 628, 756, 730, 653, 716, 833, 769, 791, 706, 841, 647, 757,\n",
       "       807, 830, 772, 631, 735, 795, 652, 753, 798, 836, 641, 773, 683,\n",
       "       856, 776, 679, 768, 809, 746, 741, 708, 764, 738, 731, 779, 733,\n",
       "       737, 732, 672, 762, 832, 745, 912, 784, 802, 755, 881, 734, 867,\n",
       "       848, 877, 861, 885, 821, 676, 704, 814, 843, 689, 690, 793, 897,\n",
       "       844, 819, 820, 697, 805, 883, 888, 743, 818, 728, 835, 875, 909,\n",
       "       720, 721, 727, 759, 896, 786, 850, 823, 845, 783, 860, 846, 797,\n",
       "       917, 849, 831, 890, 871, 837, 865, 900, 800, 788, 812, 790, 873,\n",
       "       903, 799, 816, 857, 774, 874, 930, 935, 931, 899, 852, 792, 879,\n",
       "       761, 889, 811, 763, 767, 780, 870, 803, 933, 878, 808, 851, 919,\n",
       "       891, 907, 855, 893, 936, 810, 938, 932, 827, 839, 957, 822, 882,\n",
       "       787, 925, 864, 842, 796, 928, 908, 955, 789, 926, 946, 934, 815,\n",
       "       939, 964, 983, 949, 970, 937, 956, 872, 905, 973, 824, 984, 986,\n",
       "       887, 847, 902, 863, 904, 940, 942, 826, 945, 961, 972, 969, 951,\n",
       "       898, 978, 992, 954, 854, 834, 922, 869, 952, 947, 825, 943, 968,\n",
       "       994, 876, 840, 991, 868, 859, 980, 976, 858, 995, 967, 866, 962,\n",
       "       953, 838, 990, 965, 963, 950, 892, 959, 966, 977, 862, 948, 913,\n",
       "       985, 971, 906, 944, 941, 886, 921, 895, 916, 958, 884, 910, 880,\n",
       "       960, 993, 901, 989, 894, 915, 920, 911, 998, 975, 929, 982, 927,\n",
       "       996, 974, 981, 914, 924, 923, 918, 987, 997, 979, 988, 999])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def local_shuffle(values, keys, window, seed):\n",
    "    g = np.random.Generator(np.random.PCG64(seed))\n",
    "\n",
    "    keys = np.array(keys)\n",
    "    kam_middle = keys[2 * window:] - keys[:- (2 * window)]\n",
    "    kam_prefix = keys[window: 2 * window] - keys[:window]\n",
    "    kam_suffix = keys[-window:] - keys[-2 * window: -window]\n",
    "    kam = np.hstack([kam_prefix, kam_middle, kam_suffix])\n",
    "    new_keys = keys + g.uniform(-kam, kam, size=keys.shape)\n",
    "\n",
    "    kvs = list(zip(new_keys, values))\n",
    "    kvs.sort()\n",
    "\n",
    "    return [v for _, v in kvs]\n",
    "\n",
    "\n",
    "np.array(local_shuffle(np.arange(1000), sorted(np.random.rand(1000)), 64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tensors, value=0, dtype=None):\n",
    "    dtype = tensors[0].dtype if dtype is None else dtype\n",
    "    max_len = max(tensor.shape[0] for tensor in tensors)\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        pad = (2 * len(tensor.shape)) * [0]\n",
    "        pad[-1] = max_len - tensor.shape[0]\n",
    "        new_tensors.append(F.pad(tensor, pad=pad, value=value))\n",
    "    return torch.stack(new_tensors).to(dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5]), torch.Size([3, 5]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    [0, 2, 0, 3, 0],\n",
    "    [0, 5, 0, 6, 0],\n",
    "    [0, 9, 0, 31, 31]\n",
    "]\n",
    "dur = [\n",
    "    [0, 2, 10, 1, 17],\n",
    "    [3, 1, 0, 1, 0],\n",
    "    [4, 4, 4, 0, 0],\n",
    "]\n",
    "text = torch.tensor(text, dtype=torch.long)\n",
    "mask = torch.ones_like(text)\n",
    "dur = torch.tensor(dur, dtype=torch.long)\n",
    "text.shape, mask.shape, dur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 128])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PolySpanEmb(nn.Module):\n",
    "    def __init__(self, n_vocab, d_emb, pad_id):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(n_vocab, d_emb, padding_idx=pad_id)\n",
    "\n",
    "    def forward(self, text, mask, dur):\n",
    "        lefts, rights = self._generate_sides(text)\n",
    "        lefts = self._emb(self._generate_text_rep(lefts, dur))\n",
    "        rights = self._emb(self._generate_text_rep(rights, dur))\n",
    "\n",
    "        left_c = self._generate_left_c(dur).unsqueeze_(-1)\n",
    "\n",
    "        x = left_c * lefts + (1 - left_c) * rights\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _generate_sides(self, text):\n",
    "        lefts = F.pad(text[:, :-1], [1, 0, 0, 0], value=self._emb.padding_idx)\n",
    "        lefts[:, 1::2] = text[:, 1::2]\n",
    "        rights = F.pad(text[:, 1:], [0, 1, 0, 0], value=self._emb.padding_idx)\n",
    "        rights[:, 1::2] = text[:, 1::2]\n",
    "\n",
    "        return lefts, rights\n",
    "    \n",
    "    def _generate_text_rep(self, text, dur):\n",
    "        text_rep = []\n",
    "        for t, d in zip(text, dur):\n",
    "            text_rep.append(torch.repeat_interleave(t, d))\n",
    "\n",
    "        text_rep = merge(text_rep)\n",
    "\n",
    "        return text_rep\n",
    "    \n",
    "    def _generate_left_c(self, dur):\n",
    "        x = F.pad(torch.cumsum(dur, dim=-1)[:, :-1], [1, 0], value=0)\n",
    "        pos_cm = self._generate_text_rep(x, dur)\n",
    "        mask = self._generate_text_rep(torch.ones_like(dur), dur)\n",
    "        ones_cm = torch.cumsum(mask, dim=1)\n",
    "        totals = self._generate_text_rep(dur, dur) + 1\n",
    "\n",
    "        left_c = 1 - ((ones_cm - pos_cm) * mask).float() / totals\n",
    "\n",
    "        return left_c\n",
    "\n",
    "\n",
    "pe = PolySpanEmb(32, 128, 31)\n",
    "x = pe(text, mask, dur)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 160, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = torch.rand((128, 160, 1000))\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 80, 2000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = mel.reshape(mel.shape[0], 80, -1)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0],\n",
       "        [1, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([\n",
    "    [1, 1, 0],  # mel_len was 4, becomes 2\n",
    "    [1, 0, 0],  # was 2, becomes 1; was 3\n",
    "])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sum(-1) * 2 - m.sum(-1) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5463, 0.5883, 0.8318, 0.0917, 0.1764, 0.1711, 0.7402, 0.2401, 0.6802,\n",
       "        0.4101, 0.3471, 0.1783, 0.6591, 0.5719, 0.5398, 0.4631, 0.0854, 0.0559,\n",
       "        0.0923, 0.5494, 0.1166, 0.0413, 0.3319, 0.1617, 0.6404, 0.5383, 0.5452,\n",
       "        0.6088, 0.1368, 0.1795, 0.8656, 0.0528, 0.3815, 0.8936, 0.5145, 0.9768,\n",
       "        0.2792, 0.9253, 0.7345, 0.4433, 0.7745, 0.5315, 0.6038, 0.9462, 0.9117,\n",
       "        0.1661, 0.5549, 0.9857, 0.9305, 0.9008, 0.1744, 0.0595, 0.9134, 0.3502,\n",
       "        0.0490, 0.9336, 0.1684, 0.1998, 0.8651, 0.6965, 0.5803, 0.6699, 0.3815,\n",
       "        0.8126, 0.9338, 0.8119, 0.9597, 0.8481, 0.9223, 0.2737, 0.5776, 0.4658,\n",
       "        0.9778, 0.1504, 0.6733, 0.1040, 0.4254, 0.7694, 0.1430, 0.5904, 0.7740,\n",
       "        0.8999, 0.0431, 0.1385, 0.3678, 0.4122, 0.5445, 0.4348, 0.9733, 0.4544,\n",
       "        0.4975, 0.8677, 0.3433, 0.9270, 0.0162, 0.6738, 0.6966, 0.3388, 0.9226,\n",
       "        0.7075, 0.8833, 0.8069, 0.8924, 0.4891, 0.7229, 0.5195, 0.8639, 0.1736,\n",
       "        0.7689, 0.2291, 0.6417, 0.0925, 0.6880, 0.9705, 0.3469, 0.4733, 0.2664,\n",
       "        0.9919, 0.8161, 0.7586, 0.7102, 0.9016, 0.4314, 0.5338, 0.5075, 0.1483,\n",
       "        0.8013, 0.3471, 0.0090, 0.1139, 0.7456, 0.5301, 0.8490, 0.6592, 0.8222,\n",
       "        0.4439, 0.1630, 0.3745, 0.8378, 0.0539, 0.8149, 0.7876, 0.1511, 0.2022,\n",
       "        0.0160, 0.6765, 0.6374, 0.9657, 0.0070, 0.0097, 0.2214, 0.4892, 0.2567,\n",
       "        0.8817, 0.2303, 0.6989, 0.7280, 0.8888, 0.1070, 0.0358])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel[0, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5463, 0.8759],\n",
       "        [0.8318, 0.0649],\n",
       "        [0.1764, 0.1373],\n",
       "        [0.7402, 0.4410],\n",
       "        [0.6802, 0.0086],\n",
       "        [0.3471, 0.7125],\n",
       "        [0.6591, 0.1337],\n",
       "        [0.5398, 0.2723],\n",
       "        [0.0854, 0.8415],\n",
       "        [0.0923, 0.8433],\n",
       "        [0.1166, 0.6130],\n",
       "        [0.3319, 0.3897],\n",
       "        [0.6404, 0.3436],\n",
       "        [0.5452, 0.3287],\n",
       "        [0.1368, 0.3839],\n",
       "        [0.8656, 0.1534],\n",
       "        [0.3815, 0.0053],\n",
       "        [0.5145, 0.4779],\n",
       "        [0.2792, 0.0587],\n",
       "        [0.7345, 0.3611],\n",
       "        [0.7745, 0.5071],\n",
       "        [0.6038, 0.4110],\n",
       "        [0.9117, 0.9358],\n",
       "        [0.5549, 0.2507],\n",
       "        [0.9305, 0.2080],\n",
       "        [0.1744, 0.1966],\n",
       "        [0.9134, 0.2663],\n",
       "        [0.0490, 0.5377],\n",
       "        [0.1684, 0.0659],\n",
       "        [0.8651, 0.5655],\n",
       "        [0.5803, 0.8360],\n",
       "        [0.3815, 0.5139],\n",
       "        [0.9338, 0.1661],\n",
       "        [0.9597, 0.1514],\n",
       "        [0.9223, 0.9137],\n",
       "        [0.5776, 0.9467],\n",
       "        [0.9778, 0.4361],\n",
       "        [0.6733, 0.3399],\n",
       "        [0.4254, 0.0577],\n",
       "        [0.1430, 0.0812],\n",
       "        [0.7740, 0.0336],\n",
       "        [0.0431, 0.4152],\n",
       "        [0.3678, 0.8075],\n",
       "        [0.5445, 0.4508],\n",
       "        [0.9733, 0.0398],\n",
       "        [0.4975, 0.0127],\n",
       "        [0.3433, 0.8407],\n",
       "        [0.0162, 0.5968],\n",
       "        [0.6966, 0.5904],\n",
       "        [0.9226, 0.8401],\n",
       "        [0.8833, 0.5415],\n",
       "        [0.8924, 0.3773],\n",
       "        [0.7229, 0.7615],\n",
       "        [0.8639, 0.0948],\n",
       "        [0.7689, 0.0737],\n",
       "        [0.6417, 0.5269],\n",
       "        [0.6880, 0.7140],\n",
       "        [0.3469, 0.6427],\n",
       "        [0.2664, 0.0279],\n",
       "        [0.8161, 0.1290],\n",
       "        [0.7102, 0.8259],\n",
       "        [0.4314, 0.2703],\n",
       "        [0.5075, 0.4727],\n",
       "        [0.8013, 0.6517],\n",
       "        [0.0090, 0.6710],\n",
       "        [0.7456, 0.6406],\n",
       "        [0.8490, 0.4260],\n",
       "        [0.8222, 0.5195],\n",
       "        [0.1630, 0.1977],\n",
       "        [0.8378, 0.8376],\n",
       "        [0.8149, 0.4951],\n",
       "        [0.1511, 0.6723],\n",
       "        [0.0160, 0.5698],\n",
       "        [0.6374, 0.7146],\n",
       "        [0.0070, 0.3997],\n",
       "        [0.2214, 0.1808],\n",
       "        [0.2567, 0.4998],\n",
       "        [0.2303, 0.0847],\n",
       "        [0.7280, 0.3786],\n",
       "        [0.1070, 0.6754]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0, :, :2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
